# HMM-VAE ä»£ç å®¡æŸ¥æŠ¥å‘Š

## æ€»ä½“è¯„ä»·
æ‚¨çš„ä»£ç å®ç°äº†ä¸€ä¸ªHMM-VAEæ··åˆæ¨¡å‹,ä½†ä¸è®ºæ–‡ä¸­æè¿°çš„"Partially Variational EM"æ¡†æ¶å­˜åœ¨**é‡å¤§åå·®**ã€‚

---

## ä¸»è¦é—®é¢˜åˆ†æ

### 1. **ç¼ºå¤±æ ¸å¿ƒç®—æ³•:Forward-Backwardé‡‡æ ·** âš ï¸âš ï¸âš ï¸

**è®ºæ–‡è¦æ±‚(ç¬¬10-12é¡µ):**
```
é‡‡ç”¨forward-backwardé‡‡æ ·ç­–ç•¥æ¥é‡‡æ ·éšçŠ¶æ€åºåˆ—X:

1. Forward Pass (å‰å‘ç®—æ³•):
   Î±_k(i) = p(y_{1:i}, z_{1:i}, x_i=s_k | Î¸, Î , A)
   
   åˆå§‹åŒ–: Î±_k(1) = Ï€_k Â· b_k(y_1, z_1)
   é€’æ¨: Î±_k(i) = b_k(y_i, z_i) Â· Î£_j Î±_j(i-1) Â· a_{jk}

2. Backward Sampling (åå‘é‡‡æ ·):
   - ä» x_n å¼€å§‹,æ ¹æ® Î±_k(n) é‡‡æ ·
   - ç„¶åå‘å‰é‡‡æ · x_{n-1}, ..., x_1
   p(x_i = s_k | x_{i+1} = s_j) âˆ Î±_k(i) Â· a_{kj}
```

**æ‚¨çš„ä»£ç (ç¬¬73-96è¡Œ):**
```python
def viterbi(self, x):
    # ä½¿ç”¨Viterbiç®—æ³•å¯»æ‰¾æœ€ä¼˜è·¯å¾„
    # è¿™æ˜¯ç¡®å®šæ€§çš„è§£ç ,ä¸æ˜¯é‡‡æ ·!
```

**é—®é¢˜:**
- âŒ æ‚¨ä½¿ç”¨çš„æ˜¯**Viterbiç®—æ³•**(æ‰¾æœ€ä¼˜è·¯å¾„),è®ºæ–‡è¦æ±‚çš„æ˜¯**Forward-Backwardé‡‡æ ·**(ä»åéªŒåˆ†å¸ƒé‡‡æ ·)
- âŒ Viterbiæ˜¯ç¡®å®šæ€§çš„,æ— æ³•å®ç°EMç®—æ³•ä¸­çš„Eæ­¥éª¤
- âŒ ç¼ºå°‘Î±(forward variables)çš„è®¡ç®—ç”¨äºé‡‡æ ·

**å½±å“:** è¿™å¯¼è‡´æ‚¨çš„æ¨¡å‹æ— æ³•æ­£ç¡®æ‰§è¡Œè®ºæ–‡ä¸­çš„Partially Variational EMç®—æ³•ã€‚

---

### 2. **å‘å°„æ¦‚ç‡è®¡ç®—ä¸ç¬¦åˆè®ºæ–‡å®šä¹‰** âš ï¸âš ï¸

**è®ºæ–‡å®šä¹‰(ç¬¬10é¡µ,å…¬å¼):**
```
b_k(y_i, z_i) = p(y_i, z_i | x_i = s_k, Î¸)
              = p(y_i | z_i, x_i = s_k, Î¸) Â· p(z_i)
```
å…¶ä¸­:
- `p(y_i | z_i, x_i, Î¸)`: VAEçš„è§£ç å™¨
- `p(z_i)`: æ ‡å‡†é«˜æ–¯å…ˆéªŒ N(0, I)

**æ‚¨çš„ä»£ç (ç¬¬47-58è¡Œ):**
```python
def emission_log_prob(self, x):
    # x è¿™é‡Œæ˜¯ z (latent code),ä¸æ˜¯ y (observation)
    # è®¡ç®—çš„æ˜¯ p(z | x_state),è€Œä¸æ˜¯ p(y, z | x_state)
    log_prob = -0.5 * (...ä½¿ç”¨emission_muå’Œemission_logvar...)
```

**é—®é¢˜:**
- âŒ æ‚¨çš„å‘å°„æ¦‚ç‡åªè®¡ç®—äº† `p(z | x_state)`,æ²¡æœ‰åŒ…å« `p(y | z, x_state)`
- âŒ å®Œå…¨å¿½ç•¥äº†åŸå§‹è§‚æµ‹æ•°æ®yçš„æ¡ä»¶æ¦‚ç‡
- âŒ VAEè§£ç å™¨æ²¡æœ‰å‚ä¸HMMçš„æ¨æ–­è¿‡ç¨‹

**æ­£ç¡®åº”è¯¥æ˜¯:**
```python
def compute_emission_log_prob(self, y_i, z_i, state_k):
    # 1. é€šè¿‡VAEè§£ç å™¨è®¡ç®— p(y | z, x=k)
    y_recon = self.vae.decode(z_i, condition=state_k)  # æ¡ä»¶è§£ç 
    log_p_y_given_z = -F.mse_loss(y_recon, y_i, reduction='none').sum(-1)
    
    # 2. è®¡ç®—å…ˆéªŒ p(z) = N(0, I)
    log_p_z = -0.5 * (z_i**2).sum(-1) - 0.5 * z_i.shape[-1] * np.log(2*np.pi)
    
    # 3. ç»„åˆ
    return log_p_y_given_z + log_p_z
```

---

### 3. **è®­ç»ƒæµç¨‹ä¸EMç®—æ³•ä¸ç¬¦** âš ï¸âš ï¸

**è®ºæ–‡è¦æ±‚(ç¬¬10-11é¡µ):**
```
ELBO = E_{Z~q_Ï†(Z|Y)} E_{X~p(X|Z,Y,Î¸,Î ,A)} [...]
     = L_emission(Î¸,Ï†) + L_transition(Î ,A)

è®­ç»ƒæ­¥éª¤:
1. ç”¨ç¼–ç å™¨å¾—åˆ° z_i ~ q_Ï†(z_i | y_i) å¯¹æ‰€æœ‰i
2. ç”¨Forward-Backwardé‡‡æ ·å¾—åˆ° X ~ p(X | Z, Y, Î¸, Î , A)
3. æ›´æ–°VAEå‚æ•°Î¸,Ï†å’ŒHMMå‚æ•°Î ,A
```

**æ‚¨çš„ä»£ç (ç¬¬404-446è¡Œ):**
```python
# Stage 3: Joint Training
for epoch in range(HMM_EPOCHS):
    for batch_x, in train_loader:
        recon, mu, logvar, z = vae(batch_x)
        recon_loss = F.mse_loss(recon, batch_x)
        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        hmm_nll = -hmm(z)  # è¿™é‡Œè°ƒç”¨çš„æ˜¯forwardç®—æ³•,ä¸æ˜¯é‡‡æ ·
        
        # æ²¡æœ‰é‡‡æ ·X!ç›´æ¥ä¼˜åŒ–
        total_loss = hmm_nll + 50.0 * recon_loss + 0.05 * kld + repulsion
        total_loss.backward()
```

**é—®é¢˜:**
- âŒ æ²¡æœ‰é‡‡æ ·éšçŠ¶æ€åºåˆ—X,ç›´æ¥ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–
- âŒ `hmm(z)`è®¡ç®—çš„æ˜¯forwardç®—æ³•çš„å¯¹æ•°ä¼¼ç„¶,ä¸æ˜¯ELBO
- âŒ ç¼ºå°‘EMçš„Eæ­¥éª¤(é‡‡æ ·X)
- âŒ æ‚¨çš„è®­ç»ƒæ›´åƒæ˜¯ä¸€ä¸ªè”åˆä¼˜åŒ–,è€Œä¸æ˜¯EMç®—æ³•

---

### 4. **VAEè§£ç å™¨ç¼ºå°‘æ¡ä»¶è¾“å…¥** âš ï¸

**è®ºæ–‡è¦æ±‚(ç¬¬10é¡µ):**
```
p(y_i | z_i, x_i = s_k, Î¸)
```
è§£ç å™¨å¿…é¡»ä»¥éšçŠ¶æ€x_iä½œä¸ºæ¡ä»¶

**æ‚¨çš„ä»£ç (ç¬¬104-110è¡Œ):**
```python
self.dec = nn.Sequential(
    nn.Linear(latent_dim, hidden_dim),  # åªæ¥å—zä½œä¸ºè¾“å…¥
    ...
)

def decode(self, z):
    return self.dec(z)  # æ²¡æœ‰x_stateä½œä¸ºæ¡ä»¶!
```

**é—®é¢˜:**
- âŒ è§£ç å™¨æ²¡æœ‰æ¥å—çŠ¶æ€xä½œä¸ºæ¡ä»¶è¾“å…¥
- âŒ åº”è¯¥æ˜¯ `decode(z, x_state)` å½¢å¼

**æ­£ç¡®ç¤ºä¾‹:**
```python
def decode(self, z, x_state_onehot):
    # æ‹¼æ¥zå’ŒçŠ¶æ€ä¿¡æ¯
    combined = torch.cat([z, x_state_onehot], dim=-1)
    return self.dec(combined)
```

---

### 5. **åˆå§‹åŒ–ç­–ç•¥æœ‰é—®é¢˜** âš ï¸

**æ‚¨çš„ä»£ç (ç¬¬376-402è¡Œ):**
```python
# ç”¨æ³¢åŠ¨ç‡çš„ä¸­ä½æ•°åˆ†æˆä¸¤ç±»
vol_median = np.median(vol_train)
labels = (vol_train > vol_median).astype(int)
```

**é—®é¢˜:**
- âš ï¸ è¿™ç§åˆå§‹åŒ–å¿½ç•¥äº†æ—¶åºä¾èµ–æ€§
- âš ï¸ HMMçš„çŠ¶æ€åº”è¯¥æœ‰å¹³æ»‘çš„è½¬ç§»,ä½†æ³¢åŠ¨ç‡é˜ˆå€¼ä¼šäº§ç”Ÿé¢‘ç¹è·³è·ƒ
- âš ï¸ è®ºæ–‡å»ºè®®ä½¿ç”¨èšç±»åˆå§‹åŒ–(å¦‚GMMæˆ–KMeans),è€Œä¸æ˜¯ç®€å•é˜ˆå€¼

---

### 6. **Gumbel Softmaxåœ¨å“ªé‡Œ?** âš ï¸

**è®ºæ–‡æåˆ°(ç¬¬12é¡µæœ«å°¾):**
```
"Then we can train the VAE after sampling X using Gumbel softmax mentioned above."
```

**æ‚¨çš„ä»£ç :**
- âŒ å®Œå…¨æ²¡æœ‰ä½¿ç”¨Gumbel Softmax
- âŒ Viterbiç®—æ³•æ˜¯ç¡¬åˆ†é…,ä¸æ˜¯å¯å¾®çš„è½¯é‡‡æ ·

**åº”è¯¥å®ç°:**
```python
def gumbel_softmax_sample(logits, temperature=1.0):
    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))
    y = logits + gumbel_noise
    return F.softmax(y / temperature, dim=-1)
```

---

## æ ¸å¿ƒç¼ºå¤±åŠŸèƒ½æ¸…å•

### å¿…é¡»å®ç°:
1. âœ… **Forwardç®—æ³•è®¡ç®—Î±** - æ‚¨æœ‰(åœ¨forwardå‡½æ•°ä¸­)
2. âŒ **Backwardé‡‡æ ·ç®—æ³•** - å®Œå…¨ç¼ºå¤±
3. âŒ **æ¡ä»¶VAEè§£ç å™¨** - ç¼ºå¤±çŠ¶æ€æ¡ä»¶
4. âŒ **æ­£ç¡®çš„å‘å°„æ¦‚ç‡** - å½“å‰åªè€ƒè™‘z,å¿½ç•¥y
5. âŒ **Gumbel Softmaxé‡‡æ ·** - ç¼ºå¤±
6. âŒ **EMè¿­ä»£æ¡†æ¶** - å½“å‰æ˜¯ç«¯åˆ°ç«¯ä¼˜åŒ–

---

## å¯¹è‚¡ç¥¨æ”¶ç›Šç‡é¢„æµ‹çš„å½±å“

### å½“å‰æ¶æ„çš„é—®é¢˜:
1. **çŠ¶æ€æ— æ„ä¹‰**: ç”±äºå‘å°„æ¦‚ç‡ä¸è€ƒè™‘y,HMMçŠ¶æ€ä¸æ”¶ç›Šç‡çš„å…³ç³»è¢«åˆ‡æ–­
2. **é¢„æµ‹ä¸å¯é **: Viterbiè·¯å¾„æ˜¯åŸºäºzçš„èšç±»,è€Œä¸æ˜¯yå’Œzçš„è”åˆåéªŒ
3. **æ— æ³•æ•æ‰å¸‚åœºåˆ¶åº¦**: çŠ¶æ€è½¬ç§»æ²¡æœ‰æ­£ç¡®å»ºæ¨¡,å› ä¸ºç¼ºå°‘forward-backwardé‡‡æ ·

### é¢„æµ‹æµç¨‹åº”è¯¥æ˜¯:
```python
# æµ‹è¯•æ—¶
for day_t in test_period:
    # 1. ç¼–ç å½“å¤©æ•°æ®
    z_t = encoder(y_t)
    
    # 2. æ ¹æ®æ˜¨å¤©çŠ¶æ€é¢„æµ‹ä»Šå¤©çŠ¶æ€
    state_probs = transition_matrix[prev_state]  # ä½¿ç”¨å­¦åˆ°çš„è½¬ç§»çŸ©é˜µ
    pred_state = argmax(state_probs)
    
    # 3. åœ¨é¢„æµ‹çŠ¶æ€ä¸‹ç”Ÿæˆæ”¶ç›Šç‡
    pred_returns = conditional_decoder(z_t, pred_state)  # æ¡ä»¶è§£ç 
```

æ‚¨çš„ä»£ç ç¼ºå°‘æ­¥éª¤3çš„æ¡ä»¶ç”Ÿæˆã€‚

---

## å»ºè®®çš„ä¿®æ”¹ä¼˜å…ˆçº§

### ğŸ”´ Critical (å¿…é¡»ä¿®æ”¹):
1. **å®ç°Forward-Backwardé‡‡æ ·** æ›¿æ¢Viterbi
2. **ä¿®æ”¹å‘å°„æ¦‚ç‡** åŒ…å«p(y|z,x)
3. **æ·»åŠ æ¡ä»¶è§£ç å™¨** xä½œä¸ºè¾“å…¥

### ğŸŸ¡ Important (å¼ºçƒˆå»ºè®®):
4. å®ç°Gumbel Softmaxé‡‡æ ·
5. é‡æ„è®­ç»ƒå¾ªç¯ä¸ºEMæ¡†æ¶
6. æ”¹è¿›åˆå§‹åŒ–ç­–ç•¥

### ğŸŸ¢ Nice to have:
7. æ·»åŠ æ­£åˆ™åŒ–é˜²æ­¢çŠ¶æ€åç¼©
8. å®ç°æ¸©åº¦é€€ç«
9. å¯è§†åŒ–çŠ¶æ€è½¬ç§»æ¦‚ç‡

---

## ä»£ç ä¿®æ­£ç¤ºä¾‹(æ ¸å¿ƒéƒ¨åˆ†)

### 1. Forwardç®—æ³•(è®¡ç®—Î±,ç”¨äºé‡‡æ ·):
```python
def forward_algorithm(self, y_seq, z_seq):
    """
    y_seq: (batch, seq_len, n_features) - åŸå§‹è§‚æµ‹
    z_seq: (batch, seq_len, latent_dim) - VAEç¼–ç 
    è¿”å›: alpha (batch, seq_len, n_states)
    """
    batch, seq_len, _ = y_seq.shape
    alpha = torch.zeros(batch, seq_len, self.n_states).to(y_seq.device)
    
    # åˆå§‹åŒ–: Î±_k(1) = Ï€_k Â· b_k(y_1, z_1)
    log_start = F.log_softmax(self.start_logits, dim=0)
    emission_1 = self.compute_emission_logprob(y_seq[:, 0], z_seq[:, 0])
    alpha[:, 0, :] = log_start + emission_1
    
    # é€’æ¨
    log_trans = F.log_softmax(self.trans_logits, dim=1)
    for t in range(1, seq_len):
        emission_t = self.compute_emission_logprob(y_seq[:, t], z_seq[:, t])
        for k in range(self.n_states):
            # Î±_k(t) = b_k(y_t, z_t) Â· Î£_j Î±_j(t-1) Â· a_{jk}
            trans_score = alpha[:, t-1, :] + log_trans[:, k]
            alpha[:, t, k] = torch.logsumexp(trans_score, dim=1) + emission_t[:, k]
    
    return alpha

def compute_emission_logprob(self, y, z):
    """
    è®¡ç®— log p(y, z | x_k) = log p(y | z, x_k) + log p(z)
    """
    batch = y.shape[0]
    log_probs = torch.zeros(batch, self.n_states).to(y.device)
    
    for k in range(self.n_states):
        # æ¡ä»¶è§£ç 
        state_onehot = F.one_hot(torch.tensor([k]), self.n_states).float()
        state_onehot = state_onehot.expand(batch, -1).to(y.device)
        y_recon = self.vae.decode(z, state_onehot)
        
        # log p(y | z, x=k)
        log_p_y_given_z = -F.mse_loss(y_recon, y, reduction='none').sum(-1)
        
        # log p(z) = log N(0, I)
        log_p_z = -0.5 * (z**2).sum(-1) - 0.5 * z.shape[-1] * np.log(2*np.pi)
        
        log_probs[:, k] = log_p_y_given_z + log_p_z
    
    return log_probs
```

### 2. Backwardé‡‡æ ·:
```python
def backward_sampling(self, alpha):
    """
    ä»åéªŒåˆ†å¸ƒé‡‡æ ·çŠ¶æ€åºåˆ—
    alpha: (batch, seq_len, n_states)
    è¿”å›: sampled_states (batch, seq_len)
    """
    batch, seq_len, n_states = alpha.shape
    sampled = torch.zeros(batch, seq_len, dtype=torch.long)
    
    # é‡‡æ ·æœ€åä¸€ä¸ªçŠ¶æ€
    probs_n = F.softmax(alpha[:, -1, :], dim=1)
    sampled[:, -1] = torch.multinomial(probs_n, 1).squeeze(-1)
    
    # å‘å‰é‡‡æ ·
    log_trans = F.log_softmax(self.trans_logits, dim=1)
    for t in range(seq_len-2, -1, -1):
        for b in range(batch):
            next_state = sampled[b, t+1].item()
            # p(x_t = k | x_{t+1}) âˆ Î±_k(t) Â· a_{k,next}
            logits = alpha[b, t, :] + log_trans[:, next_state]
            probs = F.softmax(logits, dim=0)
            sampled[b, t] = torch.multinomial(probs, 1).item()
    
    return sampled
```

### 3. ä¿®æ”¹è®­ç»ƒå¾ªç¯:
```python
# EMæ¡†æ¶
for epoch in range(num_epochs):
    for batch_y in dataloader:
        # E-step: é‡‡æ ·éšçŠ¶æ€
        with torch.no_grad():
            mu, logvar = vae.encode(batch_y)
            z = vae.reparameterize(mu, logvar)
            alpha = hmm.forward_algorithm(batch_y, z)
            sampled_states = hmm.backward_sampling(alpha)  # å…³é”®!
        
        # M-step: æ›´æ–°å‚æ•°
        optimizer.zero_grad()
        
        # VAEéƒ¨åˆ†
        z_new = vae.reparameterize(mu, logvar)
        state_onehot = F.one_hot(sampled_states, n_states).float()
        y_recon = vae.decode(z_new, state_onehot)
        vae_loss = F.mse_loss(y_recon, batch_y)
        
        # HMMéƒ¨åˆ†(ä½¿ç”¨é‡‡æ ·çš„çŠ¶æ€æ›´æ–°è½¬ç§»çŸ©é˜µ)
        # ... è®¡ç®—çŠ¶æ€è½¬ç§»çš„æå¤§ä¼¼ç„¶ä¼°è®¡
        
        total_loss = vae_loss + hmm_loss
        total_loss.backward()
        optimizer.step()
```

---

## æ€»ç»“

æ‚¨çš„å®ç°åœ¨ä»¥ä¸‹æ–¹é¢åç¦»äº†è®ºæ–‡:
1. âŒ ä½¿ç”¨Viterbiè€Œä¸æ˜¯Forward-Backwardé‡‡æ ·
2. âŒ å‘å°„æ¦‚ç‡å®šä¹‰é”™è¯¯
3. âŒ VAEè§£ç å™¨ç¼ºå°‘çŠ¶æ€æ¡ä»¶
4. âŒ æ²¡æœ‰å®ç°EMæ¡†æ¶
5. âŒ ç¼ºå°‘Gumbel Softmax

è¿™äº›é—®é¢˜å¯¼è‡´æ¨¡å‹æ— æ³•æ­£ç¡®å­¦ä¹ "çŠ¶æ€ä¾èµ–çš„æ”¶ç›Šç‡åˆ†å¸ƒ",ä»è€Œå½±å“é¢„æµ‹æ€§èƒ½ã€‚

å»ºè®®ä»å¤´å®ç°Forward-Backwardé‡‡æ ·å’Œæ¡ä»¶VAE,æ‰èƒ½ç¬¦åˆè®ºæ–‡çš„Partially Variational EMæ¡†æ¶ã€‚
