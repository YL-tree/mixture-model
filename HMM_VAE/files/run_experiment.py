"""
ä½¿ç”¨æ‚¨çš„ data.py è¿›è¡Œå®Œæ•´è®­ç»ƒ

è¿™ä¸ªè„šæœ¬:
1. ä½¿ç”¨æ‚¨çš„ data.py åŠ è½½æ•°æ®
2. åŒ…å«æ‰€æœ‰æ”¹è¿›(æ•°æ®æ³„æ¼ä¿®å¤ã€VAEç›‘æ§ç­‰)
3. å¦‚æœæœ‰ csi500_dataset.csv ç¼“å­˜,ç›´æ¥ä½¿ç”¨
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import os
import sys

# å¯¼å…¥æ¨¡å‹å’Œå·¥å…·
from hmm_vae_complete import (
    ConditionalVAE, HMM_ForwardBackward, EM_Trainer,
    setup_seed
)
from enhanced_visualizer import EnhancedVisualizer

# å¯¼å…¥æ‚¨çš„æ•°æ®åŠ è½½å™¨
from data import download_csi500_data


def prepare_stock_data_no_leakage(returns_data, seq_len=30, train_ratio=0.8):
    """
    å‡†å¤‡æ•°æ® (ä¿®å¤æ•°æ®æ³„æ¼)
    """
    N, D = returns_data.shape
    
    # æ—¶é—´åˆ’åˆ†
    split_idx = int(N * train_ratio)
    
    train_returns = returns_data[:split_idx]
    test_returns = returns_data[split_idx:]
    
    print(f"æ—¶é—´åˆ’åˆ†: è®­ç»ƒ [0:{split_idx}], æµ‹è¯• [{split_idx}:{N}]")
    
    # æ ‡å‡†åŒ– (åªåœ¨è®­ç»ƒé›†ä¸Šfit)
    scaler = StandardScaler()
    train_flat = np.clip(train_returns.flatten().reshape(-1, 1), -10, 10)
    scaler.fit(train_flat)
    
    train_scaled = scaler.transform(train_returns.flatten().reshape(-1, 1)).reshape(train_returns.shape)
    test_scaled = scaler.transform(test_returns.flatten().reshape(-1, 1)).reshape(test_returns.shape)
    
    # æ„å»ºåºåˆ—
    X_train = []
    for i in range(len(train_scaled) - seq_len):
        X_train.append(train_scaled[i : i + seq_len])
    X_train = np.array(X_train)
    
    X_test = []
    for i in range(len(test_scaled) - seq_len):
        X_test.append(test_scaled[i : i + seq_len])
    X_test = np.array(X_test)
    
    print(f"âœ“ æ•°æ®å‡†å¤‡å®Œæˆ: è®­ç»ƒé›†{X_train.shape}, æµ‹è¯•é›†{X_test.shape}")
    
    return X_train, X_test, scaler


def compute_pseudo_labels(X_train, n_states):
    """
    åŸºäºæ³¢åŠ¨ç‡KMeansèšç±»ç”Ÿæˆä¼ªæ ‡ç­¾

    å¯¹æ¯ä¸ªåºåˆ—çª—å£ (seq_len, n_stocks):
      1. è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„æˆªé¢æ³¢åŠ¨ç‡ std(stocks)
      2. å–åºåˆ—å†…å‡å€¼å’Œæœ€å¤§å€¼ä½œä¸ºç‰¹å¾
      3. KMeansèšç±» â†’ æ¯ä¸ªåºåˆ—ä¸€ä¸ªä¼ªæ ‡ç­¾,å¹¿æ’­åˆ°æ‰€æœ‰æ—¶é—´æ­¥

    X_train: (n_samples, seq_len, n_stocks)
    è¿”å›: pseudo_labels (n_samples, seq_len) int64
    """
    n_samples, seq_len, n_stocks = X_train.shape

    # æ¯ä¸ªåºåˆ—çš„æ³¢åŠ¨ç‡ç‰¹å¾
    # (n_samples, seq_len) â€” æ¯ä¸ªæ—¶é—´æ­¥çš„æˆªé¢æ ‡å‡†å·®
    cross_vol = np.std(X_train, axis=2)  # (n_samples, seq_len)

    # èšç±»ç‰¹å¾: å‡å€¼æ³¢åŠ¨ç‡ + æœ€å¤§æ³¢åŠ¨ç‡ + æ³¢åŠ¨ç‡å˜åŒ–å¹…åº¦
    vol_mean = cross_vol.mean(axis=1, keepdims=True)   # (n_samples, 1)
    vol_max = cross_vol.max(axis=1, keepdims=True)     # (n_samples, 1)
    vol_std = cross_vol.std(axis=1, keepdims=True)     # (n_samples, 1)
    features = np.concatenate([vol_mean, vol_max, vol_std], axis=1)  # (n_samples, 3)

    kmeans = KMeans(n_clusters=n_states, random_state=42, n_init=10)
    labels = kmeans.fit_predict(features)  # (n_samples,)

    # æŒ‰æ³¢åŠ¨ç‡å‡å€¼å¯¹ç°‡é‡æ–°ç¼–å·: 0=ä½æ³¢åŠ¨, n_states-1=é«˜æ³¢åŠ¨
    cluster_vol = [vol_mean[labels == k].mean() for k in range(n_states)]
    rank = np.argsort(cluster_vol)
    label_map = {old: new for new, old in enumerate(rank)}
    labels = np.array([label_map[l] for l in labels])

    # æ‰“å°èšç±»ç»Ÿè®¡
    print(f"\n>>> æ³¢åŠ¨ç‡ä¼ªæ ‡ç­¾èšç±» (n_states={n_states}):")
    for k in range(n_states):
        mask = labels == k
        print(f"  State {k}: {mask.sum():4d} samples, "
              f"avg_vol={vol_mean[mask].mean():.4f}, "
              f"max_vol={vol_max[mask].mean():.4f}")

    # å¹¿æ’­åˆ° (n_samples, seq_len): åŒä¸€åºåˆ—æ‰€æœ‰æ—¶é—´æ­¥å…±äº«æ ‡ç­¾
    pseudo_labels = np.repeat(labels[:, np.newaxis], seq_len, axis=1)  # (n_samples, seq_len)
    return pseudo_labels.astype(np.int64)


def pretrain_vae(vae, train_loader, device, config, visualizer):
    """VAEé¢„è®­ç»ƒ - ä½¿ç”¨æ³¢åŠ¨ç‡ä¼ªæ ‡ç­¾ + KL annealing"""
    print("\n" + "="*60)
    print("é˜¶æ®µ1: VAEé¢„è®­ç»ƒ (æ³¢åŠ¨ç‡ä¼ªæ ‡ç­¾ + KL annealing)")
    print("="*60)

    optimizer = optim.Adam(vae.parameters(), lr=config.get('lr_vae', 1e-3))

    history = {
        'recon_loss': [],
        'kl_loss': [],
        'total_loss': []
    }

    n_epochs = config.get('vae_pretrain_epochs', 50)
    kl_weight_target = config.get('kl_weight', 0.1)

    for epoch in range(n_epochs):
        vae.train()
        epoch_recon = 0
        epoch_kl = 0
        epoch_total = 0

        # KL annealing: ä»0çº¿æ€§å¢é•¿åˆ°ç›®æ ‡å€¼
        kl_weight = kl_weight_target * min(1.0, epoch / max(n_epochs * 0.5, 1))

        for batch_data in train_loader:
            batch_x = batch_data[0]
            optimizer.zero_grad()

            # ä½¿ç”¨æ³¢åŠ¨ç‡ä¼ªæ ‡ç­¾ä½œä¸ºFiLMæ¡ä»¶
            if len(batch_data) > 1:
                batch_labels = batch_data[1]  # (batch, seq_len) int64
                state_onehot = nn.functional.one_hot(
                    batch_labels, vae.n_states
                ).float()
            else:
                state_onehot = None

            recon, mu, logvar, z = vae(batch_x, state_onehot=state_onehot)

            recon_loss = nn.functional.mse_loss(recon, batch_x, reduction='mean')
            # State-Conditional KL: æŠŠzæ¨å‘å¯¹åº”çŠ¶æ€çš„å…ˆéªŒä¸­å¿ƒ
            if state_onehot is not None:
                kl_loss = vae.kl_divergence(mu, logvar, state_onehot)
            else:
                kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())

            total_loss = recon_loss + kl_weight * kl_loss

            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)
            optimizer.step()

            epoch_recon += recon_loss.item()
            epoch_kl += kl_loss.item()
            epoch_total += total_loss.item()

        n_batches = len(train_loader)
        history['recon_loss'].append(epoch_recon / n_batches)
        history['kl_loss'].append(epoch_kl / n_batches)
        history['total_loss'].append(epoch_total / n_batches)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:3d}/{n_epochs} | "
                  f"Recon: {history['recon_loss'][-1]:.4f} | "
                  f"KL: {history['kl_loss'][-1]:.4f} | "
                  f"KLw: {kl_weight:.4f} | "
                  f"Total: {history['total_loss'][-1]:.4f}")
    
    # è¯„ä¼°VAEè´¨é‡
    print("\n>>> è¯„ä¼°é¢„è®­ç»ƒVAEè´¨é‡...")
    vae.eval()
    with torch.no_grad():
        sample_x, sample_labels = [], []
        for i, batch_data in enumerate(train_loader):
            if i < 5:
                sample_x.append(batch_data[0])
                if len(batch_data) > 1:
                    sample_labels.append(batch_data[1])
            else:
                break

        all_x = torch.cat(sample_x, dim=0)
        # è¯„ä¼°æ—¶ä¹Ÿç”¨ä¼ªæ ‡ç­¾,åæ˜ çœŸå®é¢„è®­ç»ƒè´¨é‡
        if sample_labels:
            all_labels = torch.cat(sample_labels, dim=0)
            state_onehot = nn.functional.one_hot(
                all_labels, vae.n_states
            ).float()
            recon, mu, logvar, z = vae(all_x, state_onehot=state_onehot)
        else:
            recon, mu, logvar, z = vae(all_x)

        y_real = all_x.cpu().numpy()
        y_recon = recon.cpu().numpy()
        z_data = mu.cpu().numpy()[:, -1, :]

        # ä¼ªæ ‡ç­¾ç”¨äºPCAç€è‰²
        stage1_states = None
        if sample_labels:
            stage1_states = all_labels[:, -1].cpu().numpy()  # æ¯ä¸ªåºåˆ—æœ€åä¸€æ­¥çš„çŠ¶æ€

        metrics = visualizer.check_vae_quality(
            y_real, y_recon, z_data,
            stage_name="Stage1_VAE_Pretrain",
            filename="stage1_vae_quality.png",
            states=stage1_states
        )

        print(f"VAEé¢„è®­ç»ƒè´¨é‡: MSE={metrics['mse']:.6f}, RÂ²={metrics['r2']:.4f}")

    return history, metrics


def joint_training_em(vae, hmm, train_loader, device, config, visualizer):
    """EMè”åˆè®­ç»ƒ - å«VAE warm-upé˜¶æ®µ + KL annealing"""
    print("\n" + "="*60)
    print("é˜¶æ®µ2: EMè”åˆè®­ç»ƒ (å«VAE warm-up)")
    print("="*60)

    trainer = EM_Trainer(vae, hmm, device)

    history = {
        'vae_recon': [],
        'vae_kld': [],
        'hmm_nll': [],
        'total': [],
        'temperature': [],
        'state_separation': []
    }

    n_epochs = config.get('n_epochs', 100)
    em_warmup_epochs = config.get('em_warmup_epochs', 10)
    kl_weight_target = config.get('kl_weight', 0.1)
    temp_start = config.get('temperature_start', 5.0)
    temp_end = config.get('temperature_end', 0.5)
    temperature_schedule = np.linspace(temp_start, temp_end, n_epochs)

    for epoch in range(n_epochs):
        epoch_losses = {
            'vae_recon': 0,
            'vae_kld': 0,
            'hmm_nll': 0,
            'total': 0
        }

        # Change G: å‰em_warmup_epochsè½®å†»ç»“HMM,åªæ›´æ–°VAE
        is_warmup = (epoch < em_warmup_epochs)
        # Change E: EMé˜¶æ®µKL annealing
        kl_weight = kl_weight_target * min(1.0, epoch / max(n_epochs * 0.3, 1))

        for batch_data in train_loader:
            batch_x = batch_data[0]  # EMé˜¶æ®µå¿½ç•¥ä¼ªæ ‡ç­¾,ç”±HMMé‡‡æ ·çŠ¶æ€
            losses = trainer.em_step(
                batch_x,
                temperature=temperature_schedule[epoch],
                warmup=is_warmup,
                kl_weight=kl_weight,
                freeze_hmm=is_warmup
            )

            for key in epoch_losses:
                epoch_losses[key] += losses[key]

        n_batches = len(train_loader)
        for key in epoch_losses:
            history[key].append(epoch_losses[key] / n_batches)

        history['temperature'].append(temperature_schedule[epoch])

        with torch.no_grad():
            trans_matrix = hmm.get_transition_matrix().cpu().numpy()
            state_persistence = np.diag(trans_matrix).mean()
            history['state_separation'].append(state_persistence)

        if (epoch + 1) % 10 == 0:
            phase = "WARMUP" if is_warmup else "EM"
            print(f"[{phase:6s}] Epoch {epoch+1:3d}/{n_epochs} | "
                  f"Recon: {history['vae_recon'][-1]:.4f} | "
                  f"KL: {history['vae_kld'][-1]:.4f} | "
                  f"KLw: {kl_weight:.4f} | "
                  f"HMM: {history['hmm_nll'][-1]:.1f} | "
                  f"Temp: {temperature_schedule[epoch]:.2f} | "
                  f"StatePer: {state_persistence:.3f}")
    
    # è¯„ä¼°æœ€ç»ˆVAEè´¨é‡ â€” ä½¿ç”¨ViterbiçŠ¶æ€è§£ç ,è€Œéå‡åŒ€çŠ¶æ€
    print("\n>>> è¯„ä¼°æœ€ç»ˆVAEè´¨é‡ (ViterbiçŠ¶æ€è§£ç )...")
    vae.eval()
    hmm.eval()
    with torch.no_grad():
        sample_batches = []
        for i, batch_data in enumerate(train_loader):
            if i < 5:
                sample_batches.append(batch_data[0])
            else:
                break

        all_x = torch.cat(sample_batches, dim=0)

        # ç¼–ç 
        mu, logvar = vae.encode(all_x)
        z = vae.reparameterize(mu, logvar)

        # Viterbiè§£ç è·å–çŠ¶æ€åºåˆ—
        state_seq = hmm.viterbi(all_x, z)  # (n_samples, seq_len)
        state_onehot = nn.functional.one_hot(
            state_seq, vae.n_states
        ).float().to(all_x.device)

        # ç”¨ViterbiçŠ¶æ€æ¡ä»¶è§£ç 
        recon = vae.decode(z, state_onehot)

        y_real = all_x.cpu().numpy()
        y_recon = recon.cpu().numpy()
        z_data = mu.cpu().numpy()[:, -1, :]
        viterbi_states = state_seq[:, -1].cpu().numpy()  # æ¯ä¸ªåºåˆ—æœ€åä¸€æ­¥çš„çŠ¶æ€

        final_metrics = visualizer.check_vae_quality(
            y_real, y_recon, z_data,
            stage_name="Stage2_Final_VAE",
            filename="stage2_vae_quality.png",
            states=viterbi_states
        )

        # æ‰“å°æ¯ä¸ªçŠ¶æ€çš„æ ·æœ¬æ•°å’Œå„è‡ªçš„RÂ²
        print(f"æœ€ç»ˆVAEè´¨é‡: MSE={final_metrics['mse']:.6f}, RÂ²={final_metrics['r2']:.4f}")
        for s in range(vae.n_states):
            mask = viterbi_states == s
            n_s = mask.sum()
            if n_s > 0:
                y_r = y_real[mask].mean(axis=1) if len(y_real.shape) == 3 else y_real[mask]
                y_p = y_recon[mask].mean(axis=1) if len(y_recon.shape) == 3 else y_recon[mask]
                ss_res = np.sum((y_r - y_p) ** 2)
                ss_tot = np.sum((y_r - y_r.mean()) ** 2)
                r2_s = 1 - ss_res / (ss_tot + 1e-10)
                print(f"  State {s}: {n_s:4d} samples, RÂ²={r2_s:.4f}")

    return history, final_metrics


# ==========================================
# äº¤æ˜“ç­–ç•¥
# ==========================================
class TradingStrategy:
    """
    åŸºäºHMM-VAEçš„äº¤æ˜“ç­–ç•¥

    ç­–ç•¥1: çŠ¶æ€æ‹©æ—¶ - åŸºäºè®­ç»ƒé›†æ ¡å‡†çš„çŠ¶æ€ä»“ä½æƒé‡
    ç­–ç•¥2: å¤šç©ºå¯¹å†² - åšå¤šé¢„æµ‹æ”¶ç›Šé«˜çš„,åšç©ºé¢„æµ‹æ”¶ç›Šä½çš„
    """
    def __init__(self, vae, hmm, device, n_long=10, n_short=10):
        self.vae = vae
        self.hmm = hmm
        self.device = device
        self.n_long = n_long
        self.n_short = n_short
        self.state_weights = None  # æ ¡å‡†åçš„çŠ¶æ€ä»“ä½æƒé‡

    def calibrate(self, train_data, train_returns):
        """
        åœ¨è®­ç»ƒé›†ä¸Šæ ¡å‡†çŠ¶æ€-æ”¶ç›Šæ˜ å°„,é¿å…æµ‹è¯•é›†æ•°æ®çª¥æ¢

        train_data: (n_samples, seq_len, n_stocks) tensor - æ ‡å‡†åŒ–è®­ç»ƒæ•°æ®
        train_returns: (n_samples, n_stocks) numpy - å¯¹åº”çš„åŸå§‹æ”¶ç›Šç‡
        """
        print("\n>>> æ ¡å‡†çŠ¶æ€-æ”¶ç›Šæ˜ å°„ (è®­ç»ƒé›†)...")
        n_samples = len(train_data)

        # æ‰¹é‡é¢„æµ‹çŠ¶æ€ (é¿å…é€æ ·æœ¬æ¨ç†å¤ªæ…¢)
        states = self._batch_predict_states(train_data)

        # è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„å¹³å‡æ”¶ç›Š
        state_avg_returns = {}
        for s in range(self.hmm.n_states):
            mask = states == s
            if np.any(mask):
                state_avg_returns[s] = np.mean(train_returns[mask].mean(axis=1))
            else:
                state_avg_returns[s] = 0.0

        # ç”¨ softmax æŠŠæ”¶ç›Šæ˜ å°„ä¸ºä»“ä½æƒé‡ [0, 1]
        # æ­£æ”¶ç›Šçš„çŠ¶æ€è·å¾—æ›´é«˜æƒé‡,è´Ÿæ”¶ç›Šçš„çŠ¶æ€æƒé‡æ›´ä½
        returns_arr = np.array([state_avg_returns[s] for s in range(self.hmm.n_states)])
        # æ”¾å¤§å·®å¼‚: ç”¨z-scoreåsigmoid
        if returns_arr.std() > 1e-9:
            z_scores = (returns_arr - returns_arr.mean()) / returns_arr.std()
        else:
            z_scores = np.zeros_like(returns_arr)
        self.state_weights = 1.0 / (1.0 + np.exp(-z_scores * 2))  # sigmoid with scale=2

        print(f"  çŠ¶æ€æ”¶ç›Šæ˜ å°„ (è®­ç»ƒé›†):")
        for s in range(self.hmm.n_states):
            print(f"    State {s}: avg_return={state_avg_returns[s]:.4%}, "
                  f"position_weight={self.state_weights[s]:.3f}, "
                  f"samples={np.sum(states == s)}")

    def _batch_predict_states(self, data, batch_size=256):
        """æ‰¹é‡Viterbié¢„æµ‹, å–æ¯ä¸ªåºåˆ—çš„ä¼—æ•°çŠ¶æ€ (æ¯”å•å–æœ«å°¾æ›´ç¨³å®š)"""
        all_states = []
        n = len(data)
        with torch.no_grad():
            for start in range(0, n, batch_size):
                end = min(start + batch_size, n)
                batch = data[start:end]
                mu, logvar = self.vae.encode(batch)
                z = self.vae.reparameterize(mu, logvar)
                state_seq = self.hmm.viterbi(batch, z)  # (batch, seq_len)
                # å–æ¯ä¸ªåºåˆ—çš„ä¼—æ•°çŠ¶æ€,æ¯”åªçœ‹æœ€åä¸€æ­¥æ›´ç¨³å®š
                for i in range(len(batch)):
                    seq_states = state_seq[i].cpu().numpy()
                    mode_state = np.bincount(seq_states, minlength=self.hmm.n_states).argmax()
                    all_states.append(mode_state)
        return np.array(all_states)

    def predict_state(self, y_seq):
        """
        é¢„æµ‹å½“å‰å¸‚åœºçŠ¶æ€ (å–æ•´ä¸ªåºåˆ—çš„ä¼—æ•°)

        y_seq: (seq_len, n_stocks) æˆ– (1, seq_len, n_stocks)
        è¿”å›: state (int)
        """
        if len(y_seq.shape) == 2:
            y_seq = y_seq.unsqueeze(0)

        with torch.no_grad():
            mu, logvar = self.vae.encode(y_seq)
            z = self.vae.reparameterize(mu, logvar)
            state_seq = self.hmm.viterbi(y_seq, z)  # (1, seq_len)
            # ä¼—æ•°çŠ¶æ€
            seq_states = state_seq[0].cpu().numpy()
            return np.bincount(seq_states, minlength=self.hmm.n_states).argmax()

    def predict_returns(self, y_seq, state):
        """
        åœ¨ç»™å®šçŠ¶æ€ä¸‹é¢„æµ‹ä¸‹ä¸€æ—¥æ”¶ç›Šç‡

        y_seq: (seq_len, n_stocks)
        state: int
        è¿”å›: (n_stocks,) é¢„æµ‹æ”¶ç›Šç‡
        """
        if len(y_seq.shape) == 2:
            y_seq = y_seq.unsqueeze(0)

        with torch.no_grad():
            mu, _ = self.vae.encode(y_seq)
            z_last = mu[:, -1, :]

            state_onehot = torch.zeros(1, self.vae.n_states).to(self.device)
            state_onehot[0, state] = 1.0

            pred_returns = self.vae.decode(z_last, state_onehot)
            return pred_returns.squeeze(0).cpu().numpy()

    def strategy_state_timing(self, test_data, real_returns):
        """
        ç­–ç•¥1: çŠ¶æ€æ‹©æ—¶ - æ ¹æ®è®­ç»ƒé›†æ ¡å‡†çš„ä»“ä½æƒé‡è°ƒæ•´æŒä»“

        test_data: (n_samples, seq_len, n_stocks) tensor
        real_returns: (n_samples, n_stocks) numpy
        è¿”å›: nav, states, portfolio_returns
        """
        # æ‰¹é‡é¢„æµ‹çŠ¶æ€
        states = self._batch_predict_states(test_data)

        # æ‰“å°æµ‹è¯•é›†çŠ¶æ€åˆ†å¸ƒ
        print(f"\n>>> æµ‹è¯•é›†çŠ¶æ€åˆ†å¸ƒ:")
        for s in range(self.hmm.n_states):
            n_s = np.sum(states == s)
            avg_ret = real_returns[states == s].mean() if n_s > 0 else 0
            weight = self.state_weights[s] if self.state_weights is not None else "æœªæ ¡å‡†"
            print(f"  State {s}: {n_s:4d} samples, test_avg_return={avg_ret:.4%}, "
                  f"position_weight={weight}")

        # æ ¹æ®æ ¡å‡†æƒé‡ç¡®å®šä»“ä½
        portfolio_returns = []
        for i in range(len(states)):
            if self.state_weights is not None:
                weight = self.state_weights[states[i]]
            else:
                weight = 1.0  # æœªæ ¡å‡†æ—¶å…¨ä»“
            daily_return = weight * real_returns[i].mean()
            portfolio_returns.append(daily_return)

        portfolio_returns = np.array(portfolio_returns)
        nav = np.cumprod(1 + portfolio_returns)

        return nav, states, portfolio_returns

    def strategy_long_short(self, test_data, real_returns):
        """
        ç­–ç•¥2: å¤šç©ºå¯¹å†² - åšå¤šTop N / åšç©ºBottom N

        test_data: (n_samples, seq_len, n_stocks) tensor
        real_returns: (n_samples, n_stocks) numpy
        è¿”å›: nav, positions_history, portfolio_returns
        """
        n_samples = len(test_data)
        portfolio_returns = []
        positions_history = []

        for i in range(n_samples):
            state = self.predict_state(test_data[i])
            pred_ret = self.predict_returns(test_data[i], state)

            top_long = np.argsort(pred_ret)[-self.n_long:]
            top_short = np.argsort(pred_ret)[:self.n_short]

            long_return = real_returns[i, top_long].mean()
            short_return = -real_returns[i, top_short].mean()

            total_return = (long_return + short_return) / 2
            portfolio_returns.append(total_return)

            positions_history.append({
                'long': top_long.tolist(),
                'short': top_short.tolist(),
                'state': state
            })

        portfolio_returns = np.array(portfolio_returns)
        nav = np.cumprod(1 + portfolio_returns)

        return nav, positions_history, portfolio_returns


# ==========================================
# å›æµ‹è¯„ä¼°
# ==========================================
def calculate_metrics(strategy_returns, benchmark_returns):
    """è®¡ç®—ç»©æ•ˆæŒ‡æ ‡"""
    n_days = len(strategy_returns)
    total_return = (1 + strategy_returns).prod() - 1
    annual_return = (1 + total_return) ** (252 / max(n_days, 1)) - 1

    annual_vol = strategy_returns.std() * np.sqrt(252)

    sharpe = (strategy_returns.mean() / (strategy_returns.std() + 1e-9)) * np.sqrt(252)

    cum_returns = np.cumprod(1 + strategy_returns)
    running_max = np.maximum.accumulate(cum_returns)
    drawdown = (cum_returns - running_max) / running_max
    max_drawdown = drawdown.min()

    win_rate = np.mean(strategy_returns > 0)

    excess_returns = strategy_returns - benchmark_returns
    ir = (excess_returns.mean() / (excess_returns.std() + 1e-9)) * np.sqrt(252)

    calmar = annual_return / abs(max_drawdown) if max_drawdown != 0 else 0

    return {
        'annual_return': annual_return,
        'annual_vol': annual_vol,
        'sharpe': sharpe,
        'max_drawdown': max_drawdown,
        'win_rate': win_rate,
        'information_ratio': ir,
        'calmar': calmar,
        'total_return': total_return
    }


def print_metrics(metrics, strategy_name):
    """æ‰“å°æŒ‡æ ‡"""
    print(f"\n{strategy_name} ç»©æ•ˆæŒ‡æ ‡:")
    print(f"  æ€»æ”¶ç›Šç‡:        {metrics['total_return']:.2%}")
    print(f"  å¹´åŒ–æ”¶ç›Šç‡:      {metrics['annual_return']:.2%}")
    print(f"  å¹´åŒ–æ³¢åŠ¨ç‡:      {metrics['annual_vol']:.2%}")
    print(f"  å¤æ™®æ¯”ç‡:        {metrics['sharpe']:.4f}")
    print(f"  ä¿¡æ¯æ¯”ç‡:        {metrics['information_ratio']:.4f}")
    print(f"  Calmaræ¯”ç‡:      {metrics['calmar']:.4f}")
    print(f"  æœ€å¤§å›æ’¤:        {metrics['max_drawdown']:.2%}")
    print(f"  èƒœç‡:            {metrics['win_rate']:.2%}")


def plot_comparison(nav1, nav2, nav_bench, ret1, ret2, ret_bench, states):
    """å›æµ‹å¯¹æ¯”å¯è§†åŒ–"""
    fig = plt.figure(figsize=(16, 12))
    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)

    # 1. å‡€å€¼æ›²çº¿å¯¹æ¯”
    ax1 = fig.add_subplot(gs[0, :])
    ax1.plot(nav1, label='Strategy 1: State Timing', linewidth=2)
    ax1.plot(nav2, label='Strategy 2: Long-Short', linewidth=2)
    ax1.plot(nav_bench, label='Benchmark: Buy & Hold', linewidth=2, alpha=0.7, linestyle='--')
    ax1.set_title('NAV Comparison', fontsize=14, fontweight='bold')
    ax1.legend(loc='best', fontsize=10)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylabel('NAV')

    # 2. æ—¥æ”¶ç›Šç‡åˆ†å¸ƒ
    ax2 = fig.add_subplot(gs[1, 0])
    ax2.hist(ret1, bins=50, alpha=0.6, label='Strategy 1', density=True)
    ax2.hist(ret_bench, bins=50, alpha=0.6, label='Benchmark', density=True)
    ax2.set_title('Return Distribution (S1 vs Benchmark)', fontsize=12)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_xlabel('Daily Return')

    ax3 = fig.add_subplot(gs[1, 1])
    ax3.hist(ret2, bins=50, alpha=0.6, label='Strategy 2', color='green', density=True)
    ax3.hist(ret_bench, bins=50, alpha=0.6, label='Benchmark', density=True)
    ax3.set_title('Return Distribution (S2 vs Benchmark)', fontsize=12)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_xlabel('Daily Return')

    # 3. å›æ’¤æ›²çº¿
    ax4 = fig.add_subplot(gs[2, 0])
    dd1 = (nav1 - np.maximum.accumulate(nav1)) / np.maximum.accumulate(nav1)
    dd2 = (nav2 - np.maximum.accumulate(nav2)) / np.maximum.accumulate(nav2)
    dd_bench = (nav_bench - np.maximum.accumulate(nav_bench)) / np.maximum.accumulate(nav_bench)

    ax4.fill_between(range(len(dd1)), dd1, 0, alpha=0.5, label='Strategy 1')
    ax4.fill_between(range(len(dd2)), dd2, 0, alpha=0.5, label='Strategy 2')
    ax4.plot(dd_bench, label='Benchmark', linewidth=2, alpha=0.7, linestyle='--')
    ax4.set_title('Drawdown', fontsize=12)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    ax4.set_ylabel('Drawdown')

    # 4. çŠ¶æ€æ—¶åº
    ax5 = fig.add_subplot(gs[2, 1])
    ax5.plot(states, drawstyle='steps-post', linewidth=2, color='purple')
    ax5.set_title('Predicted Market States', fontsize=12)
    ax5.set_ylabel('State')
    ax5.set_xlabel('Time')
    ax5.grid(True, alpha=0.3)

    plt.savefig('backtest_comparison.png', dpi=150, bbox_inches='tight')
    print(">>> Saved backtest_comparison.png")
    plt.close()


def comprehensive_backtest(strategy, test_data, real_returns):
    """å…¨é¢å›æµ‹åˆ†æ"""
    print("\n" + "="*60)
    print("å›æµ‹åˆ†æ")
    print("="*60)

    # ç­–ç•¥1: çŠ¶æ€æ‹©æ—¶
    print("\n### ç­–ç•¥1: çŠ¶æ€æ‹©æ—¶ ###")
    nav1, states, ret1 = strategy.strategy_state_timing(test_data, real_returns)

    # åŸºå‡†: ä¹°å…¥æŒæœ‰
    benchmark_ret = real_returns.mean(axis=1)
    nav_benchmark = np.cumprod(1 + benchmark_ret)

    metrics1 = calculate_metrics(ret1, benchmark_ret)
    print_metrics(metrics1, "çŠ¶æ€æ‹©æ—¶")

    # ç­–ç•¥2: å¤šç©ºå¯¹å†²
    print("\n### ç­–ç•¥2: å¤šç©ºå¯¹å†² (Top10 Long + Top10 Short) ###")
    nav2, positions, ret2 = strategy.strategy_long_short(test_data, real_returns)

    metrics2 = calculate_metrics(ret2, benchmark_ret)
    print_metrics(metrics2, "å¤šç©ºå¯¹å†²")

    # å¯è§†åŒ–
    plot_comparison(nav1, nav2, nav_benchmark, ret1, ret2, benchmark_ret, states)

    return {
        'strategy1': {'nav': nav1, 'returns': ret1, 'states': states, 'metrics': metrics1},
        'strategy2': {'nav': nav2, 'returns': ret2, 'positions': positions, 'metrics': metrics2},
        'benchmark': {'nav': nav_benchmark, 'returns': benchmark_ret}
    }


def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨æ‚¨çš„æ•°æ®"""
    # è®¾ç½®
    setup_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}\n")
    
    # é…ç½®
    config = {
        'seq_len': 30,
        'latent_dim': 32,
        'n_states': 3,
        'batch_size': 128,
        'train_ratio': 0.8,
        # VAEé¢„è®­ç»ƒ
        'vae_pretrain_epochs': 50,
        'lr_vae': 1e-3,
        'kl_weight': 0.1,
        # EMè®­ç»ƒ
        'n_epochs': 100,
        'em_warmup_epochs': 10,
        'temperature_start': 5.0,
        'temperature_end': 0.5
    }
    
    print("é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"  {key:20s}: {value}")
    
    # ==========================================
    # åŠ è½½æ‚¨çš„æ•°æ®
    # ==========================================
    print("\n" + "="*60)
    print("åŠ è½½æ•°æ® (ä½¿ç”¨æ‚¨çš„ data.py)")
    print("="*60)
    
    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶
    cache_path = "csi500_dataset.csv"
    
    if os.path.exists(cache_path):
        print(f"âœ“ å‘ç°ç¼“å­˜æ–‡ä»¶: {cache_path}")
    else:
        print(f"âŒ æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {cache_path}")
        print(f"è¯·å…ˆè¿è¡Œ data.py ä¸‹è½½æ•°æ®,æˆ–å°†æ‚¨çš„æ•°æ®æ–‡ä»¶é‡å‘½åä¸º {cache_path}")
        return None
    
    # ä½¿ç”¨æ‚¨çš„å‡½æ•°åŠ è½½æ•°æ®
    df = download_csi500_data(
        start_date="20160101",
        end_date="20231231",
        cache_path=cache_path
    )
    
    if df is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥!")
        return None
    
    # è½¬æ¢ä¸ºnumpy
    returns_data = df.values.astype(np.float32)
    
    print(f"\nâœ“ æ•°æ®åŠ è½½æˆåŠŸ")
    print(f"  å½¢çŠ¶: {returns_data.shape}")
    print(f"  æ—¥æœŸèŒƒå›´: {df.index[0]} åˆ° {df.index[-1]}")
    
    # æ•°æ®ç»Ÿè®¡
    returns_flat = returns_data.flatten()
    print(f"\næ•°æ®ç»Ÿè®¡:")
    print(f"  å¹³å‡æ”¶ç›Šç‡: {returns_flat.mean():.6f}")
    print(f"  æ ‡å‡†å·®: {returns_flat.std():.6f}")
    print(f"  æœ€å°å€¼: {returns_flat.min():.6f}")
    print(f"  æœ€å¤§å€¼: {returns_flat.max():.6f}")
    
    # ==========================================
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    # ==========================================
    print("\n" + "="*60)
    print("æ•°æ®é¢„å¤„ç†")
    print("="*60)
    
    X_train, X_test, scaler = prepare_stock_data_no_leakage(
        returns_data,
        seq_len=config['seq_len'],
        train_ratio=config['train_ratio']
    )
    
    n_stocks = X_train.shape[2]

    # è®¡ç®—æ³¢åŠ¨ç‡ä¼ªæ ‡ç­¾
    pseudo_labels = compute_pseudo_labels(X_train, n_states=config['n_states'])

    # è½¬æ¢ä¸ºTensor
    X_train_tensor = torch.from_numpy(X_train).float().to(device)
    X_test_tensor = torch.from_numpy(X_test).float().to(device)
    pseudo_labels_tensor = torch.from_numpy(pseudo_labels).long().to(device)

    train_loader = DataLoader(
        TensorDataset(X_train_tensor, pseudo_labels_tensor),
        batch_size=config['batch_size'],
        shuffle=True
    )
    
    # ==========================================
    # åˆå§‹åŒ–æ¨¡å‹
    # ==========================================
    print("\n>>> åˆå§‹åŒ–æ¨¡å‹...")
    vae = ConditionalVAE(
        input_dim=n_stocks,
        latent_dim=config['latent_dim'],
        n_states=config['n_states']
    ).to(device)
    
    hmm = HMM_ForwardBackward(
        n_states=config['n_states'],
        latent_dim=config['latent_dim'],
        vae=vae
    ).to(device)
    
    visualizer = EnhancedVisualizer()
    
    # ==========================================
    # è®­ç»ƒ
    # ==========================================
    # é˜¶æ®µ1: VAEé¢„è®­ç»ƒ
    vae_history, vae_metrics = pretrain_vae(
        vae, train_loader, device, config, visualizer
    )
    
    # é˜¶æ®µ2: EMè”åˆè®­ç»ƒ
    em_history, final_metrics = joint_training_em(
        vae, hmm, train_loader, device, config, visualizer
    )
    
    # ==========================================
    # ç”Ÿæˆå¯è§†åŒ–
    # ==========================================
    print("\n>>> ç”Ÿæˆè®­ç»ƒç›‘æ§é¢æ¿...")
    visualizer.plot_training_dashboard(em_history, filename="training_dashboard.png")
    
    # çŠ¶æ€è½¬ç§»çŸ©é˜µ
    import seaborn as sns
    trans_matrix = hmm.get_transition_matrix().detach().cpu().numpy()
    
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(trans_matrix, annot=True, fmt='.3f', cmap='YlOrRd',
                xticklabels=[f'S{i}' for i in range(config['n_states'])],
                yticklabels=[f'S{i}' for i in range(config['n_states'])],
                vmin=0, vmax=1, ax=ax)
    ax.set_title('HMM State Transition Matrix', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('transition_matrix.png', dpi=150)
    plt.close()
    print("âœ“ Saved transition matrix")
    
    # ==========================================
    # é˜¶æ®µ3: ç­–ç•¥å›æµ‹
    # ==========================================
    print("\n" + "="*60)
    print("é˜¶æ®µ3: ç­–ç•¥å›æµ‹")
    print("="*60)

    # å¯¹é½æµ‹è¯•é›†çœŸå®æ”¶ç›Šç‡
    # åºåˆ— i è¦†ç›– [split_idx+i, split_idx+i+seq_len), æœ€åä¸€å¤©æ˜¯ T = split_idx+i+seq_len-1
    # T æ—¥æ”¶ç›˜åç”¨è¯¥åºåˆ—é¢„æµ‹çŠ¶æ€ â†’ å†³å®š T+1 æ—¥ä»“ä½ â†’ èµš/äº T+1 æ—¥æ”¶ç›Š
    # æ‰€ä»¥ real_returns åº”å– T+1 = split_idx + i + seq_len
    split_idx = int(len(returns_data) * config['train_ratio'])
    seq_len = config['seq_len']

    # T+1 æ—¥çš„åŸå§‹æ”¶ç›Šç‡
    real_returns_test = []
    n_test_samples = X_test_tensor.shape[0]
    for i in range(n_test_samples):
        day_idx = split_idx + i + seq_len  # T+1
        if day_idx < len(returns_data):
            real_returns_test.append(returns_data[day_idx])
        else:
            break
    real_returns_test = np.array(real_returns_test)

    # æˆªæ–­åˆ°ç›¸åŒé•¿åº¦
    min_len = min(len(X_test_tensor), len(real_returns_test))
    X_test_backtest = X_test_tensor[:min_len]
    real_returns_test = real_returns_test[:min_len]

    print(f"å›æµ‹æ ·æœ¬æ•°: {min_len}")
    print(f"æµ‹è¯•é›†æ”¶ç›Šç‡å‡å€¼: {real_returns_test.mean():.6f}")

    # åˆå§‹åŒ–ç­–ç•¥
    strategy = TradingStrategy(
        vae, hmm, device,
        n_long=10,
        n_short=10
    )

    # æ ¡å‡†: åœ¨è®­ç»ƒé›†ä¸Šå»ºç«‹çŠ¶æ€-æ”¶ç›Šæ˜ å°„
    # è®­ç»ƒåºåˆ— i è¦†ç›– [i, i+seq_len), T+1 = i+seq_len
    real_returns_train = []
    n_train_samples = X_train_tensor.shape[0]
    for i in range(n_train_samples):
        day_idx = i + seq_len  # T+1: ç”¨ T æ—¥æ”¶ç›˜é¢„æµ‹, èµš T+1 æ—¥æ”¶ç›Š
        if day_idx < split_idx:
            real_returns_train.append(returns_data[day_idx])
        else:
            break
    real_returns_train = np.array(real_returns_train)
    X_train_calib = X_train_tensor[:len(real_returns_train)]

    strategy.calibrate(X_train_calib, real_returns_train)

    # æ‰§è¡Œå›æµ‹
    backtest_results = comprehensive_backtest(
        strategy,
        X_test_backtest,
        real_returns_test
    )

    # ==========================================
    # æ€»ç»“
    # ==========================================
    print("\n" + "="*60)
    print("å…¨éƒ¨å®Œæˆ!")
    print("="*60)
    print(f"\né¢„è®­ç»ƒVAE: MSE={vae_metrics['mse']:.6f}, RÂ²={vae_metrics['r2']:.4f}")
    print(f"æœ€ç»ˆVAE:   MSE={final_metrics['mse']:.6f}, RÂ²={final_metrics['r2']:.4f}")
    print(f"æ”¹è¿›: RÂ² {vae_metrics['r2']:.4f} -> {final_metrics['r2']:.4f} "
          f"({(final_metrics['r2'] - vae_metrics['r2']) * 100:+.2f}%)")

    s1_m = backtest_results['strategy1']['metrics']
    s2_m = backtest_results['strategy2']['metrics']
    print(f"\nçŠ¶æ€æ‹©æ—¶: Sharpe={s1_m['sharpe']:.4f}, "
          f"MaxDD={s1_m['max_drawdown']:.2%}, "
          f"Annual={s1_m['annual_return']:.2%}")
    print(f"å¤šç©ºå¯¹å†²: Sharpe={s2_m['sharpe']:.4f}, "
          f"MaxDD={s2_m['max_drawdown']:.2%}, "
          f"Annual={s2_m['annual_return']:.2%}")

    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  stage1_vae_quality.png  - VAEé¢„è®­ç»ƒè´¨é‡")
    print("  stage2_vae_quality.png  - æœ€ç»ˆVAEè´¨é‡")
    print("  training_dashboard.png  - è®­ç»ƒç›‘æ§é¢æ¿")
    print("  transition_matrix.png   - çŠ¶æ€è½¬ç§»çŸ©é˜µ")
    print("  backtest_comparison.png - å›æµ‹å¯¹æ¯”å›¾")

    # ä¿å­˜æ¨¡å‹
    torch.save({
        'vae_state_dict': vae.state_dict(),
        'hmm_state_dict': hmm.state_dict(),
        'config': config,
        'vae_metrics': vae_metrics,
        'final_metrics': final_metrics,
        'backtest_metrics': {
            'state_timing': s1_m,
            'long_short': s2_m
        }
    }, 'hmm_vae_model.pth')
    print("  hmm_vae_model.pth      - è®­ç»ƒå¥½çš„æ¨¡å‹")

    return vae, hmm, (vae_history, em_history), (X_train_tensor, X_test_tensor, scaler), backtest_results


if __name__ == "__main__":
    result = main()

    if result is not None:
        vae, hmm, history, data, backtest = result
        print("\nè®­ç»ƒæˆåŠŸ! æ¨¡å‹å’Œå›æµ‹ç»“æœå·²ä¿å­˜ã€‚")
    else:
        print("\nè®­ç»ƒå¤±è´¥,è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ã€‚")