import torch
import os
import json
import numpy as np
import torch.nn.functional as F
from scipy.optimize import linear_sum_assignment
from sklearn.metrics import normalized_mutual_info_score as NMI

# å¯¼å…¥ä½ çš„æ¨¡åž‹å®šä¹‰
from common_dpm import Config, get_semi_loaders
from mDPM import mDPM_SemiSup  # ç¡®ä¿ mDPM.py ä¸­åŒ…å« mDPM_SemiSup ç±»

# ==========================================
# æ ¸å¿ƒè¯„ä¼°å‡½æ•° (åŒ…å« Low-T å’Œ Monte Carlo ç­–ç•¥)
# ==========================================
def robust_evaluate(model, loader, cfg):
    """
    V3 å¼ºåŠ›è¯„ä¼°ç‰ˆï¼š
    1. ä¸“æ³¨äºŽä½Žæ—¶é—´æ­¥ (Low t strategy) - ä¿¡å·æœ€å¼ºåŒºåŸŸ
    2. è’™ç‰¹å¡æ´›å¤šæ¬¡é‡‡æ · (Monte Carlo Averaging) - æ¶ˆé™¤å™ªå£°æ–¹å·®
    """
    model.eval()
    preds, ys_true = [], []
    
    # [å…³é”®ç­–ç•¥ 1] åªè¯„ä¼°ä¸­ä½Žæ—¶é—´æ­¥
    # é¿å¼€ t > 500 çš„é«˜å™ªå£°åŒºåŸŸ
    eval_timesteps = [50, 100, 150, 200] 
    
    # [å…³é”®ç­–ç•¥ 2] å¯¹æ¯ä¸ª t é‡å¤é‡‡æ ·æ¬¡æ•°
    n_repeats = 5 
    
    print(f"ðŸ” å¼€å§‹è¯„ä¼°: TimeSteps={eval_timesteps}, Repeats={n_repeats}")

    with torch.no_grad():
        for i, (x_0, y_true) in enumerate(loader):
            x_0 = x_0.to(cfg.device)
            batch_size = x_0.size(0)
            
            # (Batch, Num_Classes)
            cumulative_mse = torch.zeros(batch_size, cfg.num_classes, device=cfg.device)
            
            for t_val in eval_timesteps:
                mse_t_sum = torch.zeros(batch_size, cfg.num_classes, device=cfg.device)
                
                for _ in range(n_repeats):
                    # 1. é‡‡æ ·å™ªå£°
                    noise = torch.randn_like(x_0)
                    current_t = torch.full((batch_size,), t_val, device=cfg.device, dtype=torch.long)
                    x_t = model.dpm_process.q_sample(x_0, current_t, noise)
                    
                    # 2. å¯¹æ¯ä¸ªç±»åˆ«è®¡ç®— MSE
                    for k in range(cfg.num_classes):
                        y_onehot_k = F.one_hot(torch.full((batch_size,), k, device=x_0.device),
                                               num_classes=cfg.num_classes).float()
                        
                        pred_noise = model.cond_denoiser(x_t, current_t, y_onehot_k)
                        
                        # è®¡ç®—å•ä¸ªæ ·æœ¬çš„ MSE
                        loss_k = F.mse_loss(pred_noise, noise, reduction='none').view(batch_size, -1).mean(dim=1)
                        mse_t_sum[:, k] += loss_k
                
                # å¹³å‡è¯¥æ—¶é—´æ­¥çš„ MSE å¹¶ç´¯åŠ 
                cumulative_mse += (mse_t_sum / n_repeats)

            # MSE è¶Šå°ï¼Œä¼¼ç„¶æ¦‚çŽ‡è¶Šå¤§ã€‚Logits = -MSE
            log_pi = torch.log(model.registered_pi + 1e-8).unsqueeze(0).to(x_0.device)
            final_logits = -cumulative_mse + log_pi
            
            pred_cluster = torch.argmax(final_logits, dim=1).cpu().numpy()
            preds.append(pred_cluster)
            ys_true.append(y_true.numpy())
            
            if i % 10 == 0:
                print(f"   å¤„ç† Batch {i}...")

    preds = np.concatenate(preds)
    ys_true = np.concatenate(ys_true)
    
    # --- æŒ‡æ ‡è®¡ç®— ---
    nmi = NMI(ys_true, preds)
    
    # åŒˆç‰™åˆ©ç®—æ³•å¯¹é½ (å“ªæ€•æ˜¯å…¨ç›‘ç£ä¹Ÿå¯ä»¥è·‘ä¸€ä¸‹ï¼Œç¡®è®¤æ˜¯å¦å¯¹é½)
    n_classes = cfg.num_classes
    cost_matrix = np.zeros((n_classes, n_classes))
    for i in range(n_classes):
        for j in range(n_classes):
            cost_matrix[i, j] = -np.sum((ys_true == i) & (preds == j))
            
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    cluster2label = {int(c): int(l) for c, l in zip(col_ind, row_ind)}
    aligned_preds = np.array([cluster2label.get(p, 0) for p in preds])
    
    # åŽŸå§‹å‡†ç¡®çŽ‡ (å‡è®¾ç±»åˆ« ID ä¸€ä¸€å¯¹åº”)
    raw_acc = np.mean(preds == ys_true)
    # å¯¹é½åŽå‡†ç¡®çŽ‡
    posterior_acc = np.mean(aligned_preds == ys_true)
    
    return raw_acc, posterior_acc, nmi, cluster2label

# ==========================================
# ä¸»åŠ è½½é€»è¾‘
# ==========================================
def load_and_run():
    # 1. åˆå§‹åŒ–é…ç½®
    cfg = Config()
    
    # [é‡è¦] å¿…é¡»ä¸Žè®­ç»ƒæ—¶çš„é…ç½®ä¸€è‡´ï¼Œå¦åˆ™æ¨¡åž‹æƒé‡åŠ è½½ä¼šæŠ¥é”™
    # å¦‚æžœä½ åœ¨è®­ç»ƒæ—¶ä¿®æ”¹äº† batch_size æˆ– channelsï¼Œè¿™é‡Œä¹Ÿè¦æ”¹
    cfg.unet_base_channels = 32  # è¯·ç¡®è®¤ä½ è®­ç»ƒæ—¶æ˜¯ç”¨ 32 è¿˜æ˜¯ 64
    cfg.batch_size = 32          # è¯„ä¼°æ—¶ Batch å¯ä»¥å°ä¸€ç‚¹ä»¥é˜²æ˜¾å­˜æº¢å‡º
    
    # æ¨¡åž‹è·¯å¾„
    model_path = os.path.join(cfg.output_dir, "mDPM_best_model.pt") 
    # æˆ–è€…å¦‚æžœä½ æ˜¯åœ¨å…¨ç›‘ç£æ–‡ä»¶å¤¹ä¸‹ï¼š
    # model_path = "./mDPM_results_supervised/mDPM_best_model.pt" 
    
    if not os.path.exists(model_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡åž‹æ–‡ä»¶: {model_path}")
        return

    print(f"ðŸ“‚ æ­£åœ¨åŠ è½½æ¨¡åž‹: {model_path}")
    print(f"âš™ï¸  è®¾å¤‡: {cfg.device}")

    # 2. åˆå§‹åŒ–æ¨¡åž‹æž¶æž„
    model = mDPM_SemiSup(cfg).to(cfg.device)
    
    # 3. åŠ è½½æƒé‡
    try:
        checkpoint = torch.load(model_path, map_location=cfg.device)
        model.load_state_dict(checkpoint)
        print("âœ… æ¨¡åž‹æƒé‡åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        print("æç¤ºï¼šè¯·æ£€æŸ¥ cfg.unet_base_channels æ˜¯å¦ä¸Žè®­ç»ƒæ—¶ä¸€è‡´ã€‚")
        return

    # 4. èŽ·å–éªŒè¯é›†æ•°æ®
    # get_semi_loaders è¿”å›ž (labeled, unlabeled, val_loader)
    _, _, val_loader = get_semi_loaders(cfg)
    
    # 5. è¿è¡Œè¯„ä¼°
    print("\nðŸš€ å¼€å§‹è¿è¡Œ Robust Evaluate...")
    raw_acc, post_acc, nmi, mapping = robust_evaluate(model, val_loader, cfg)
    
    print("\n" + "="*30)
    print(f"ðŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æžœ")
    print("="*30)
    print(f"Raw Accuracy (æ— å¯¹é½):  {raw_acc:.4f}")
    print(f"Aligned Accuracy (å¯¹é½åŽ): {post_acc:.4f}")
    print(f"NMI Score:              {nmi:.4f}")
    print("-" * 30)
    print(f"ç±»åˆ«æ˜ å°„ (Cluster -> Label): {mapping}")
    print("="*30)

if __name__ == "__main__":
    load_and_run()