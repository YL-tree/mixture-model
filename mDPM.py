# mDPM.py
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import optuna
import os
import gc  # æ˜¾å¼å†…å­˜ç®¡ç†
import numpy as np
from scipy.optimize import linear_sum_assignment
from torchvision.utils import save_image
import itertools  # <--- å¿…é¡»åŠ è¿™ä¸ª
# å¯¼å…¥ common ç»„ä»¶
from common_dpm import *

# -----------------------
# Model Definition (mDPM Adaptation)
# -----------------------
class mDPM_SemiSup(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.cond_denoiser = ConditionalUnet(
            in_channels=cfg.image_channels,
            base_channels=cfg.unet_base_channels,
            num_classes=cfg.num_classes,
            time_emb_dim=cfg.unet_time_emb_dim
        )
        self.dpm_process = DPMForwardProcess(
            timesteps=cfg.timesteps,
            schedule='linear',
            image_channels=cfg.image_channels
        )
        # ç±»åˆ«åˆ†å¸ƒå…ˆéªŒ (Uniform initialization)
        self.register_buffer('registered_pi', torch.ones(cfg.num_classes) / cfg.num_classes)
        
    def estimate_posterior_logits(self, x_0, cfg, scale_factor=1.0):
        batch_size = x_0.size(0)
        num_classes = cfg.num_classes
        M = cfg.posterior_sample_steps
        
        # ä½¿ç”¨è´Ÿ MSE ç´¯åŠ å™¨ (Log Likelihood âˆ -MSE)
        accum_neg_mse = torch.zeros(batch_size, num_classes, device=x_0.device)
        
        with torch.no_grad():
            for _ in range(M):
                # é‡‡æ ·æ—¶é—´æ­¥ï¼Œå»ºè®®è¦†ç›–ä¸­é—´å¤§éƒ¨åˆ†åŒºåŸŸ
                t = torch.randint(100, 900, (batch_size,), device=x_0.device).long()
                noise = torch.randn_like(x_0)
                x_t = self.dpm_process.q_sample(x_0, t, noise)
                
                # è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„æ¡ä»¶å»å™ªè¯¯å·®
                for k in range(num_classes):
                    y_cond = torch.full((batch_size,), k, device=x_0.device, dtype=torch.long)
                    y_onehot = F.one_hot(y_cond, num_classes=num_classes).float()
                    pred_noise = self.cond_denoiser(x_t, t, y_onehot)
                    
                    # MSE (Batch,)
                    mse = F.mse_loss(pred_noise, noise, reduction='none').view(batch_size, -1).mean(dim=1)
                    accum_neg_mse[:, k] += -mse

        # å¹³å‡åŒ– Monte Carlo æ­¥æ•°
        avg_neg_mse = accum_neg_mse / M
        
        # åŠ å…¥ Prior: log P(y) + Scale * log P(x|y)
        # é™åˆ¶ log_pi é˜²æ­¢æ— ç©·å°
        log_pi = torch.log(torch.clamp(self.registered_pi, min=1e-6)).unsqueeze(0)
        
        # [é€»è¾‘ä¿®å¤] å»é™¤ Z-Scoreï¼Œä½¿ç”¨ç›´æ¥ç¼©æ”¾
        # Scale å¾ˆå¤§ (e.g. 100) å› ä¸º MSE å·®å€¼å¾ˆå° (e.g. 0.01)
        final_logits = log_pi + (avg_neg_mse * scale_factor)

        return final_logits

    def forward(self, x_0, cfg, y=None, current_scale=100.0, current_lambda=0.05):
        """
        å‰å‘ä¼ æ’­åŒ…å« E-Step å’Œ M-Step çš„æŸå¤±è®¡ç®—
        """
        batch_size = x_0.size(0)

        # -------------------
        # ç›‘ç£æ¨¡å¼ (Labeled Data)
        # -------------------
        if y is not None:
            t = torch.randint(0, cfg.timesteps, (batch_size,), device=x_0.device).long()
            noise = torch.randn_like(x_0)
            x_t = self.dpm_process.q_sample(x_0, t, noise)
            
            y_onehot = F.one_hot(y, num_classes=cfg.num_classes).float()
            pred_noise = self.cond_denoiser(x_t, t, y_onehot)
            dpm_loss = F.mse_loss(pred_noise, noise, reduction='mean') 
            
            return dpm_loss, -dpm_loss.item(), dpm_loss.item(), 0.0, None, None
            
        # -------------------
        # æ— ç›‘ç£æ¨¡å¼ (Unlabeled Data) - æ”¹ä¸º Gumbel-Softmax
        # -------------------
        else:
            # === E-Step: æ¨æ–­æ½œå˜é‡ y çš„åˆ†å¸ƒ ===
            # è·å– Logits (æ³¨æ„ï¼šScale ä¾ç„¶å¾ˆé‡è¦ï¼ŒLogits å·®å¼‚è¿‡å°ä¼šå¯¼è‡´ Gumbel è¾“å‡ºè¶‹å‘å‡åŒ€)
            logits = self.estimate_posterior_logits(x_0, cfg, scale_factor=current_scale)
            
            # ä»…ç”¨äºæŒ‡æ ‡ç›‘æ§ (Softmax)
            resp = F.softmax(logits, dim=1) 

            
            # === M-Step: Gumbel-Softmax Sampling ===
            # [å…³é”®ä¿®æ”¹] ä½¿ç”¨å¯å¯¼çš„è½¯é‡‡æ ·
            # hard=False è¡¨ç¤ºæˆ‘ä»¬éœ€è¦è½¯æ¦‚ç‡å‘é‡ (e.g., [0.1, 0.8, 0.1]) è€Œä¸æ˜¯ One-hot
            # è¿™æ ·æ¢¯åº¦å¯ä»¥æµå‘æ‰€æœ‰ç±»åˆ«ï¼Œå¦‚æœä¸ç¡®å®šæ˜¯ç±»2è¿˜æ˜¯ç±»7ï¼Œä¸¤ä¸ªç±»éƒ½ä¼šå¾—åˆ°æ›´æ–°
            # [æ–°å¢] è®¡ç®—ç½®ä¿¡åº¦æƒé‡
            # è·å–æ¯ä¸ªæ ·æœ¬æœ€å¤§çš„æ¦‚ç‡å€¼ (B,)
            max_probs, _ = resp.max(dim=1)
            
            # [å…³é”®ç­–ç•¥] åªæœ‰ç½®ä¿¡åº¦ > 0.4 çš„æ ·æœ¬æ‰è´¡çŒ® Loss
            # æˆ–è€…æ˜¯è½¯æƒé‡: weight = max_probs^2 (è®©ç¡®ä¿¡çš„æ ·æœ¬æƒé‡æ›´å¤§)
            mask = (max_probs > 0.4).float() 

            y_soft = F.gumbel_softmax(logits, tau=gumbel_temp, hard=False)
            
            t_train = torch.randint(0, cfg.timesteps, (batch_size,), device=x_0.device).long()
            noise = torch.randn_like(x_0)
            x_t_train = self.dpm_process.q_sample(x_0, t_train, noise)
            
            pred_noise = self.cond_denoiser(x_t_train, t_train, y_soft)
            
            # è®¡ç®— element-wise MSE: (B, C, H, W) -> (B,)
            loss_per_sample = F.mse_loss(pred_noise, noise, reduction='none').mean(dim=[1,2,3])
            
            # [å…³é”®] åº”ç”¨ Maskï¼Œåªè®­ç»ƒé«˜è´¨é‡æ ·æœ¬
            # åŠ ä¸Šä¸€ä¸ªæå°å€¼é˜²æ­¢é™¤ä»¥0
            dpm_loss = (loss_per_sample * mask).sum() / (mask.sum() + 1e-8)
        
            
            # === è¾…åŠ©æŸå¤±: ç†µæ­£åˆ™åŒ– ===
            entropy = -(resp * torch.log(resp + 1e-8)).sum(dim=1).mean()
            
            total_loss = dpm_loss + current_lambda * entropy

            # === Update Prior (Momentum) ===
            if self.training:
                with torch.no_grad():
                    # ç¨å¾®å‡æ…¢æ›´æ–°é€Ÿåº¦ï¼Œé˜²æ­¢åˆæœŸæ³¢åŠ¨
                    momentum = 0.995 
                    current_counts = resp.mean(0).detach()
                    self.registered_pi.copy_(momentum * self.registered_pi + (1 - momentum) * current_counts)
            
            return total_loss, -total_loss.item(), dpm_loss.item(), entropy.item(), resp.detach(), None

        # -------------------
        # æ— ç›‘ç£æ¨¡å¼ (Unlabeled Data) - Soft-EM with Dynamic Annealing
        # -------------------
        # else:
        #     # === E-Step: æ¨æ–­æ½œå˜é‡ x çš„åˆ†å¸ƒ ===
        #     # [ä¿®æ”¹] ä¼ å…¥åŠ¨æ€ Scale
        #     logits = self.estimate_posterior_logits(x_0, cfg, scale_factor=current_scale)
            
        #     # ä½¿ç”¨ Softmax è·å–æ¦‚ç‡ (Soft-EM)
        #     # åœ¨æ¨ç†é˜¶æ®µï¼Œç›´æ¥ç”¨ Softmax æ¯” Gumbel æ›´ç¨³å®šï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨ Logits å±‚é¢åŠ äº† scale
        #     resp = F.softmax(logits, dim=1)
            
        #     # === M-Step: è®­ç»ƒå»å™ªç½‘ç»œ ===
        #     t_train = torch.randint(0, cfg.timesteps, (batch_size,), device=x_0.device).long()
        #     noise = torch.randn_like(x_0)
        #     x_t_train = self.dpm_process.q_sample(x_0, t_train, noise)
            
        #     weighted_dpm_loss = 0.0
            
        #     # è®¡ç®—åŠ æƒ Loss
        #     for k in range(cfg.num_classes):
        #         y_onehot_k = F.one_hot(torch.full((batch_size,), k, device=x_0.device),
        #                                num_classes=cfg.num_classes).float()
                
        #         pred_noise_k = self.cond_denoiser(x_t_train, t_train, y_onehot_k)
        #         dpm_loss_k = F.mse_loss(pred_noise_k, noise, reduction='none').view(batch_size, -1).mean(dim=1)
                
        #         weighted_dpm_loss += (resp[:, k].detach() * dpm_loss_k).mean()
            
        #     # === è¾…åŠ©æŸå¤± ===
        #     # [ä¿®æ”¹] ä½¿ç”¨åŠ¨æ€ Lambda
        #     entropy = -(resp * torch.log(resp + 1e-8)).sum(dim=1).mean()
        #     total_loss = weighted_dpm_loss + current_lambda * entropy

        #     # --- æ›´æ–° Prior ---
        #     if self.training:
        #         with torch.no_grad():
        #             current_counts = resp.mean(0)
        #             self.registered_pi.copy_(0.99 * self.registered_pi + 0.01 * current_counts)
            
        #     return total_loss, -total_loss.item(), weighted_dpm_loss.item(), entropy.item(), resp.detach(), None
        # æ— ç›‘ç£éƒ¨åˆ†
        if y is None:
            # === E-Step ===
            logits = self.estimate_posterior_logits(x_0, cfg, scale_factor=current_scale)
            resp = F.softmax(logits, dim=1) # shape: (B, K)
            
            # === M-Step (ä½¿ç”¨ Hard Sampling ä»¥ç¬¦åˆè®ºæ–‡å¹¶åŠ é€Ÿ) ===
            # è®ºæ–‡: "By drawing samples... obtain class labels... proceed with noise prediction"
            
            # 1. é‡‡æ ·ä¼ªæ ‡ç­¾
            pseudo_y = torch.multinomial(resp, 1).squeeze(1) # (B,)
            
            # 2. æ„é€ è®­ç»ƒæ•°æ®
            t_train = torch.randint(0, cfg.timesteps, (batch_size,), device=x_0.device).long()
            noise = torch.randn_like(x_0)
            x_t_train = self.dpm_process.q_sample(x_0, t_train, noise)
            
            y_onehot_pseudo = F.one_hot(pseudo_y, num_classes=cfg.num_classes).float()
            
            # 3. è®¡ç®— DPM Loss (åªç®—ä¸€æ¬¡ forwardï¼Œä¸ç”¨ç®— K æ¬¡)
            pred_noise = self.cond_denoiser(x_t_train, t_train, y_onehot_pseudo)
            dpm_loss = F.mse_loss(pred_noise, noise, reduction='mean')
            
            # 4. è¾…åŠ© Loss (Entropy Regularization)
            entropy = -(resp * torch.log(resp + 1e-8)).sum(dim=1).mean()
            
            total_loss = dpm_loss + current_lambda * entropy

            # æ›´æ–° Prior (Momentum Update)
            if self.training:
                with torch.no_grad():
                    # è¿™é‡Œå¯ä»¥ç”¨ resp çš„å‡å€¼ï¼Œä¹Ÿå¯ä»¥ç”¨ pseudo_y çš„ one-hot å‡å€¼ï¼Œresp æ›´å¹³æ»‘
                    self.registered_pi.copy_(0.99 * self.registered_pi + 0.01 * resp.mean(0).detach())
            
            return total_loss, -total_loss.item(), dpm_loss.item(), entropy.item(), resp.detach(), None
            
# -----------------------
# Evaluation Utils
# -----------------------
def evaluate_model(model, loader, cfg):
    try:
        from scipy.optimize import linear_sum_assignment
    except ImportError:
        print("Scipy not found, skipping detailed evaluation.")
        return 0.0, {}, 0.0

    model.eval()
    preds, ys_true = [], []
    
    # é»„é‡‘åŒºé—´è¯„ä¼°
    eval_timesteps = [300, 500, 700] 
    n_repeats = 5
    
    with torch.no_grad():
        for x_0, y_true in loader:
            x_0 = x_0.to(cfg.device)
            batch_size = x_0.size(0)
            
            cumulative_mse = torch.zeros(batch_size, cfg.num_classes, device=cfg.device)
            
            for t_val in eval_timesteps:
                mse_t_sum = torch.zeros(batch_size, cfg.num_classes, device=cfg.device)
                
                for _ in range(n_repeats):
                    noise = torch.randn_like(x_0)
                    current_t = torch.full((batch_size,), t_val, device=cfg.device, dtype=torch.long)
                    x_t = model.dpm_process.q_sample(x_0, current_t, noise)
                    
                    for k in range(cfg.num_classes):
                        y_vec = F.one_hot(torch.full((batch_size,), k, device=x_0.device), cfg.num_classes).float()
                        pred_noise = model.cond_denoiser(x_t, current_t, y_vec)
                        
                        loss = F.mse_loss(pred_noise, noise, reduction='none').view(batch_size, -1).mean(dim=1)
                        mse_t_sum[:, k] += loss
                
                cumulative_mse += (mse_t_sum / n_repeats)

            pred_cluster = torch.argmin(cumulative_mse, dim=1).cpu().numpy()
            preds.append(pred_cluster)
            ys_true.append(y_true.numpy())

    preds = np.concatenate(preds)
    ys_true = np.concatenate(ys_true)
    
    # --- è®¡ç®—æŒ‡æ ‡ ---
    nmi = NMI(ys_true, preds)
    
    n_classes = cfg.num_classes
    cost_matrix = np.zeros((n_classes, n_classes))
    for i in range(n_classes):
        for j in range(n_classes):
            cost_matrix[i, j] = -np.sum((ys_true == i) & (preds == j))
            
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    cluster2label = {int(c): int(l) for c, l in zip(col_ind, row_ind)}
    aligned_preds = np.array([cluster2label.get(p, 0) for p in preds])
    posterior_acc = np.mean(aligned_preds == ys_true)
    
    return posterior_acc, cluster2label, nmi

def sample_and_save_dpm(denoiser, dpm_process, num_classes, out_path, device, n_per_class=10):
    """ä¿®æ­£åçš„ DPM é€†å‘é‡‡æ ·è¿‡ç¨‹"""
    T = dpm_process.timesteps
    denoiser.eval()
    image_c = dpm_process.image_channels

    with torch.no_grad():
        shape = (n_per_class * num_classes, image_c, 28, 28)
        x_t = torch.randn(shape, device=device)
        y_cond = torch.arange(num_classes).to(device).repeat_interleave(n_per_class).long()
        
        for i in reversed(range(0, T)):
            t = torch.full((shape[0],), i, device=device, dtype=torch.long)
            
            alpha_t = dpm_process._extract_t(dpm_process.alphas, t, shape)
            one_minus_alpha_t_bar = dpm_process._extract_t(dpm_process.sqrt_one_minus_alphas_cumprod, t, shape)
            pred_noise = denoiser(x_t, t, y_cond)
            mu_t_1 = (x_t - (1 - alpha_t) / one_minus_alpha_t_bar * pred_noise) / alpha_t.sqrt()
            sigma_t_1 = dpm_process._extract_t(dpm_process.posterior_variance, t, shape).sqrt()
            
            if i > 0:
                noise = torch.randn_like(x_t)
            else:
                noise = torch.zeros_like(x_t)
            x_t = mu_t_1 + sigma_t_1 * noise

        save_image(x_t.clamp(-1, 1), out_path, nrow=n_per_class, normalize=True, value_range=(-1, 1))
    print(f"ğŸ’¾ Saved DPM samples to {out_path}")

# -----------------------
# Training Engine
# -----------------------
def run_training_session(model, optimizer, labeled_loader, unlabeled_loader, val_loader, cfg,
                         is_final_training=False, trial_id=None, resume_path=None):
    
    total_epochs = cfg.final_epochs if is_final_training else cfg.optuna_epochs
    sample_dir = os.path.join(cfg.output_dir, "sample_progress")
    os.makedirs(sample_dir, exist_ok=True)

    start_epoch = 1
    best_val_nmi = -np.inf
    metrics = {"Loss": [], "NMI": [], "Acc": []}

    # ==========================================
    # ğŸ”„ Resume Logic (æ–­ç‚¹ç»­è®­)
    # ==========================================
    if resume_path and os.path.exists(resume_path):
        print(f"ğŸ”„ Resuming from: {resume_path}")
        checkpoint = torch.load(resume_path, map_location=cfg.device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        if 'optimizer_state_dict' in checkpoint and optimizer is not None:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
        start_epoch = checkpoint['epoch'] + 1
        best_val_nmi = checkpoint.get('best_nmi', -np.inf)
        print(f"âœ… Resumed at Epoch {start_epoch}, Best NMI: {best_val_nmi:.4f}")
    else:
        if resume_path:
            print(f"âš ï¸ Checkpoint not found at {resume_path}, starting from scratch.")

    # ==========================================
    # Detect Mode (æ¨¡å¼æ£€æµ‹)
    # ==========================================
    mode = "UNKNOWN"
    if labeled_loader is not None and unlabeled_loader is not None: 
        mode = "SEMI_SUPERVISED"
    elif labeled_loader is not None: 
        mode = "SUPERVISED"
        cfg.alpha_unlabeled = 0.0
    elif unlabeled_loader is not None: 
        mode = "UNSUPERVISED"
    
    print(f"ğŸš€ Training Mode: {mode}")

    # ==========================================
    # Training Loop
    # ==========================================
    for epoch in range(start_epoch, total_epochs + 1):
        # --- Schedulers (å…³é”®å‚æ•°è°ƒåº¦) ---
        progress = (epoch - 1) / total_epochs # 0.0 -> 1.0
        
        # 1. Dynamic Scale: æ”¾å¤§ Logits å·®å¼‚
        # [ä¿®æ”¹] æé«˜èµ·å§‹å€¼åˆ° 150ï¼Œé˜²æ­¢åˆæœŸ Logits å¤ªå¹³æ»‘å¯¼è‡´ Gumbel ä¹Ÿæ˜¯å‡åŒ€åˆ†å¸ƒ
        dynamic_scale = 300.0 + (600.0 - 300.0) * progress
        
        # 2. Dynamic Lambda: ç†µæƒ©ç½š
        if epoch < 10: 
            dynamic_lambda = 0.0
        else: 
            dynamic_lambda = 0.0 + (0.2) * ((epoch - 10) / (max(1, total_epochs - 10)))
        
        # 3. [æ–°å¢] Gumbel Temperature: æ¢ç´¢ -> ç¡®å®š
        # åˆæœŸ 1.0 (å¹³æ»‘ï¼Œæ¢ç´¢æ‰€æœ‰ç±»)ï¼ŒåæœŸ 0.5 (å°–é”ï¼Œæ¥è¿‘ One-hot)
        gumbel_temp = 1.0 - (0.5 * progress)
        
        if epoch % 5 == 0 or epoch == 1:
            print(f"ğŸ”¥ [Scheduler] Ep {epoch}: Scale={dynamic_scale:.1f}, Lambda={dynamic_lambda:.4f}, Temp={gumbel_temp:.2f}")

        model.train()
        loss_accum = 0.0
        n_batches = 0
        
        # Warm-up alpha (å‰5è½®ä¸è¿›è¡Œæ— ç›‘ç£è®­ç»ƒï¼Œå…ˆå­¦å¥½ backbone)
        current_alpha_un = cfg.alpha_unlabeled
        if mode == "SEMI_SUPERVISED" and epoch <= 5: 
            current_alpha_un = 0.0
        
        # Iterator Setup
        if mode == "SEMI_SUPERVISED": 
            iterator = zip(itertools.cycle(labeled_loader), unlabeled_loader)
        elif mode == "SUPERVISED": 
            iterator = ((batch, None) for batch in labeled_loader)
        elif mode == "UNSUPERVISED": 
            iterator = ((None, batch) for batch in unlabeled_loader)

        for batch_lab, batch_un in iterator:
            optimizer.zero_grad()
            total_loss = torch.tensor(0.0, device=cfg.device)
            resp = None 

            # --- A. Supervised Part ---
            if batch_lab is not None:
                x_lab, y_lab = batch_lab
                x_lab, y_lab = x_lab.to(cfg.device), y_lab.to(cfg.device).long()
                loss_lab, _, _, _, _, _ = model(x_lab, cfg, y=y_lab) # y is provided
                total_loss += loss_lab

            # --- B. Unsupervised Part (Gumbel-Softmax) ---
            if batch_un is not None and current_alpha_un > 0:
                # [ä¿®å¤] æ­£ç¡®è§£åŒ…ï¼Œè·å– y_un_true ç”¨äº Debug
                x_un, y_un_true = batch_un 
                x_un = x_un.to(cfg.device)
                y_un_true = y_un_true.to(cfg.device)
                
                # è°ƒç”¨ forwardï¼Œä¼ å…¥ gumbel_temp
                loss_un, _, _, _, resp, _ = model(x_un, cfg, y=None, 
                                                  current_scale=dynamic_scale, 
                                                  current_lambda=dynamic_lambda,
                                                  gumbel_temp=gumbel_temp) # <--- ä¼ å…¥æ¸©åº¦
                
                total_loss += current_alpha_un * loss_un
                
                # === æ·±åº¦ç›‘æ§ (Deep Monitoring) ===
                # æ¯ 50 ä¸ª batch æ‰“å°ä¸€æ¬¡ï¼Œè§‚å¯Ÿæ˜¯å¦å‘ç”Ÿ Mode Collapse
                if n_batches % 50 == 0:
                    with torch.no_grad():
                        # resp æ˜¯ Softmax åçš„æ¦‚ç‡ï¼Œç”¨äºè§‚å¯Ÿæ¨¡å‹"æƒ³"é€‰å“ªä¸ª
                        pseudo_labels = resp.argmax(dim=1)
                        
                        # è®¡ç®—ä¼ªæ ‡ç­¾å‡†ç¡®ç‡ (ä»…ä¾›å‚è€ƒï¼Œä¸å‚ä¸æ¢¯åº¦)
                        acc_unsup = (pseudo_labels == y_un_true).float().mean().item()
                        
                        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
                        conf = resp.max(dim=1)[0].mean().item()
                        
                        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ (æœ€é‡è¦ï¼æ£€æŸ¥æ˜¯å¦å…¨éƒ¨åˆ†åˆ°äº†æŸä¸€ç±»)
                        class_counts = torch.bincount(pseudo_labels, minlength=cfg.num_classes).cpu().numpy()
                        
                        print(f"   [Debug] Unsup Acc: {acc_unsup:.2f} | Conf: {conf:.2f} | Dist: {class_counts}")

            # Optimization
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            loss_accum += total_loss.item()
            n_batches += 1

        # ==========================================
        # Validation & Checkpointing
        # ==========================================
        # è¿™é‡Œçš„éªŒè¯æ¯”è¾ƒè€—æ—¶ï¼Œä½†å¾ˆæœ‰å¿…è¦
        raw_acc, _, val_nmi = evaluate_model(model, val_loader, cfg)
        target_metric = raw_acc if mode == "SUPERVISED" else val_nmi
        
        # å‡†å¤‡ Checkpoint æ•°æ®
        checkpoint_dict = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_nmi': best_val_nmi,
            'config': cfg.__dict__
        }
        
        # 1. ä¿å­˜ "Latest" (ç”¨äº Resume)
        last_ckpt_path = os.path.join(cfg.output_dir, "checkpoint_last.pt")
        torch.save(checkpoint_dict, last_ckpt_path)

        # 2. ä¿å­˜ "Periodic" (æ¯10è½®å¤‡ä»½ï¼Œé˜²æ­¢è·‘å´©äº†æ²¡åæ‚”è¯)
        if epoch % 10 == 0:
            periodic_path = os.path.join(cfg.output_dir, f"checkpoint_epoch_{epoch:03d}.pt")
            torch.save(checkpoint_dict, periodic_path)
            print(f"   ğŸ’¾ [Backup] Periodic checkpoint saved: {periodic_path}")

        # 3. ä¿å­˜ "Best" (æœ€ä½³æŒ‡æ ‡)
        if target_metric > best_val_nmi:
            best_val_nmi = target_metric
            if is_final_training:
                torch.save(model.state_dict(), os.path.join(cfg.output_dir, "best_model.pt"))
                torch.save(checkpoint_dict, os.path.join(cfg.output_dir, "checkpoint_best.pt"))
                print(f"   â˜… New Best Model Saved! (NMI: {best_val_nmi:.4f})")

        # Log
        log_tag = "FINAL" if is_final_training else f"TRIAL-{trial_id}"
        print(f"[{log_tag}] Ep {epoch} | Loss: {loss_accum/n_batches:.4f} | Acc: {raw_acc:.4f} | NMI: {val_nmi:.4f}")

        # é‡‡æ ·çœ‹å›¾
        if is_final_training and (epoch % 10 == 0 or epoch == total_epochs):
            sample_and_save_dpm(model.cond_denoiser, model.dpm_process, cfg.num_classes,
                                os.path.join(sample_dir, f"epoch_{epoch:03d}.png"), cfg.device)
    
    return best_val_nmi, metrics

def main():
    # [å¼€å…³] æ˜¯å¦æ–­ç‚¹ç»­è®­
    RESUME_TRAINING = True  
    
    cfg = Config()
    print("="*30)
    print(f"--- Starting Training (M={cfg.posterior_sample_steps}) ---")
    print(f"Config: LR={cfg.lr}, Scale Range=50->300")
    print("="*30)

    model = mDPM_SemiSup(cfg).to(cfg.device)
    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)
    labeled_loader, unlabeled_loader, val_loader = get_semi_loaders(cfg)
    
    resume_path = os.path.join(cfg.output_dir, "checkpoint_last.pt") if RESUME_TRAINING else None
    
    run_training_session(
        model, optimizer, labeled_loader, unlabeled_loader, val_loader, cfg, 
        is_final_training=True,
        resume_path=resume_path
    )
    
    torch.save(model.state_dict(), os.path.join(cfg.output_dir, "final_model.pt"))
    print(f"âœ… Done. Results saved to {cfg.output_dir}")

if __name__ == "__main__":
    main()