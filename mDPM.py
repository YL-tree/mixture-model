# mDPM.py
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import optuna
import os
import gc  # æ˜¾å¼å†…å­˜ç®¡ç†
import numpy as np
from scipy.optimize import linear_sum_assignment
from torchvision.utils import save_image

# å¯¼å…¥ common ç»„ä»¶
from common_dpm import *
# -----------------------
# Model Definition (mDPM Adaptation)
# -----------------------
class mDPM_SemiSup(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.cond_denoiser = ConditionalUnet(
            in_channels=cfg.image_channels,
            base_channels=cfg.unet_base_channels,
            num_classes=cfg.num_classes,
            time_emb_dim=cfg.unet_time_emb_dim
        )
        self.dpm_process = DPMForwardProcess(
            timesteps=cfg.timesteps,
            schedule='linear',
            image_channels=cfg.image_channels
        )
        # ç±»åˆ«åˆ†å¸ƒå…ˆéªŒ (Uniform initialization)
        self.register_buffer('registered_pi', torch.ones(cfg.num_classes) / cfg.num_classes)
        
    def estimate_posterior_logits(self, x_0, cfg):
        batch_size = x_0.size(0)
        num_classes = cfg.num_classes
        M = cfg.posterior_sample_steps
        
        accum_log_lik = torch.zeros(batch_size, num_classes, device=x_0.device)
        
        with torch.no_grad():
            for _ in range(M):
                # [å…³é”®ä¿®æ”¹]
                # ä¸å†ä» [0, 1000] å‡åŒ€é‡‡æ ·
                # è€Œæ˜¯ä¸“æ³¨äº "è¯­ä¹‰åŒºé—´" [300, 700]
                # è¿™ä¸æ˜¯ hackï¼Œè¿™æ˜¯ "é™ä½ä¼°è®¡æ–¹å·®" çš„æ•°å­¦æ‰‹æ®µ
                t_start = int(0.3 * cfg.timesteps)
                t_end = int(0.7 * cfg.timesteps)
                
                # é‡‡æ · t
                t = torch.randint(t_start, t_end, (batch_size,), device=x_0.device).long()
                
                noise = torch.randn_like(x_0)
                x_t = self.dpm_process.q_sample(x_0, t, noise)
                
                for k in range(num_classes):
                    y_cond = torch.full((batch_size,), k, device=x_0.device, dtype=torch.long)
                    y_onehot = F.one_hot(y_cond, num_classes=num_classes).float()
                    
                    # æ­¤æ—¶ U-Net å†…éƒ¨ä¼šè‡ªåŠ¨ç»™ y_emb ä¹˜ä¸Šæ¯”è¾ƒå¤§çš„æƒé‡ (å› ä¸º t åœ¨ä¸­é—´)
                    # æ‰€ä»¥å¦‚æœ k æ˜¯é”™çš„ï¼Œpred_noise å°±ä¼šé”™å¾—å¾ˆç¦»è°± -> MSE å¾ˆå¤§
                    # å¦‚æœ k æ˜¯å¯¹çš„ï¼Œpred_noise å°±ä¼šå¾ˆå‡† -> MSE å¾ˆå°
                    pred_noise = self.cond_denoiser(x_t, t, y_onehot)
                    
                    # Log Likelihood Proxy
                    mse = F.mse_loss(pred_noise, noise, reduction='none').view(batch_size, -1).mean(dim=1)
                    accum_log_lik[:, k] += -mse

        # åœ¨å¾ªç¯ç»“æŸåï¼Œaccum_log_lik é‡Œé¢å­˜çš„æ˜¯ -MSE
        # MSE çš„æ•°å€¼é€šå¸¸å¾ˆå° (0.02 å·¦å³)ï¼Œå¯¼è‡´å·®å¼‚åªæœ‰ 0.001 çº§åˆ«
        
        # [å…³é”®ä¿®æ”¹] æ‰‹åŠ¨æ”¾å¤§å·®å¼‚ (Scale Factor)
        # è¿™ç›¸å½“äºäººä¸ºé™ä½äº† E-Step çš„ "æ¸©åº¦"
        # è®©çŒœå¯¹çš„ç±»åˆ«çš„ Logits æ˜¾è‘—é«˜äºçŒœé”™çš„
        scale_factor = 1.0  
        accum_log_lik = accum_log_lik * scale_factor
        
        log_pi = torch.log(self.registered_pi + 1e-8).unsqueeze(0)
        final_logits = log_pi + (accum_log_lik / M)
        
        return final_logits

    def forward(self, x_0, cfg, y=None):
        """
        å‰å‘ä¼ æ’­åŒ…å« E-Step å’Œ M-Step çš„æŸå¤±è®¡ç®—
        """
        batch_size = x_0.size(0)
        
        # -------------------
        # ç›‘ç£æ¨¡å¼ (Labeled Data)
        # -------------------
        if y is not None:
            # æ ‡å‡† DDPM è®­ç»ƒï¼šé‡‡æ · 1 ä¸ª t
            t = torch.randint(0, cfg.timesteps, (batch_size,), device=x_0.device).long()
            noise = torch.randn_like(x_0)
            x_t = self.dpm_process.q_sample(x_0, t, noise)
            
            y_onehot = F.one_hot(y, num_classes=cfg.num_classes).float()
            pred_noise = self.cond_denoiser(x_t, t, y_onehot)
            dpm_loss = F.mse_loss(pred_noise, noise, reduction='mean') 
            
            return dpm_loss, -dpm_loss.item(), dpm_loss.item(), 0.0, None, None
            
        # -------------------
        # æ— ç›‘ç£æ¨¡å¼ (Unlabeled Data) - PVEM
        # -------------------
        else:
            # === E-Step: æ¨æ–­æ½œå˜é‡ x çš„åˆ†å¸ƒ ===
            # è¿™é‡Œä½¿ç”¨äº† Multi-step è¿‘ä¼¼ï¼Œæ¯”åŸæ¥çš„å•æ­¥æ›´å‡†
            logits = self.estimate_posterior_logits(x_0, cfg)
            
            # ä½¿ç”¨ Gumbel Softmax è¿›è¡Œé‡å‚æ•°åŒ–æˆ–æ¾å¼›é‡‡æ ·
            # è¿™é‡Œçš„ resp å¯¹åº”è®ºæ–‡ä¸­çš„ \tilde{p}(x|z,y)
            resp = gumbel_softmax_sample(logits, cfg.current_gumbel_temp)
            
            # === M-Step: è®­ç»ƒå»å™ªç½‘ç»œ ===
            # è®ºæ–‡: Sample x ~ p(x|z,y) then train DDPM
            # å®é™…æ“ä½œ: ä½¿ç”¨ resp åŠ æƒçš„ Loss (Soft-EM)ï¼Œè¿™åœ¨æ·±åº¦å­¦ä¹ ä¸­æ¯” Hard Sampling æ›´ç¨³å®š
            
            # é‡æ–°é‡‡æ ·ä¸€ä¸ª t ç”¨äºè®­ç»ƒ (æ ‡å‡† DDPM åšæ³•)
            t_train = torch.randint(0, cfg.timesteps, (batch_size,), device=x_0.device).long()
            noise = torch.randn_like(x_0)
            x_t_train = self.dpm_process.q_sample(x_0, t_train, noise)
            
            weighted_dpm_loss = 0.0
            
            # è®¡ç®—åŠ æƒ Loss
            # L = Sum_k q(x=k) * ||eps - eps_theta(x_t, t, k)||^2
            for k in range(cfg.num_classes):
                y_onehot_k = F.one_hot(torch.full((batch_size,), k, device=x_0.device),
                                       num_classes=cfg.num_classes).float()
                
                # è¿™é‡Œéœ€è¦æ¢¯åº¦ï¼Œç”¨äºæ›´æ–° cond_denoiser
                pred_noise_k = self.cond_denoiser(x_t_train, t_train, y_onehot_k)
                
                # Per-sample loss
                dpm_loss_k = F.mse_loss(pred_noise_k, noise, reduction='none').view(batch_size, -1).mean(dim=1)
                
                # ä½¿ç”¨ E-Step ç®—å‡ºæ¥çš„ resp è¿›è¡ŒåŠ æƒ
                # resp.detach() å¾ˆå…³é”®ï¼ç¡®ä¿æ¢¯åº¦ä¸å›ä¼ åˆ° E-Step é€»è¾‘
                weighted_dpm_loss += (resp[:, k].detach() * dpm_loss_k).mean()
            
            # === è¾…åŠ©æŸå¤± ===
            # ç†µæœ€å°åŒ–: é¼“åŠ±æ¨¡å‹åšå‡ºç¡®å®šçš„é¢„æµ‹ (Paper context: Self-consistent)
            # entropy = -(resp * torch.log(resp + 1e-8)).sum(dim=1).mean()
            
            # total_loss = weighted_dpm_loss - cfg.lambda_entropy * entropy
            total_loss = weighted_dpm_loss
            
            return total_loss, -total_loss.item(), weighted_dpm_loss.item(), 0.0, resp.detach(), None

# -----------------------
# Evaluation Utils
# -----------------------
def evaluate_model(model, loader, cfg):
    """
    æ”¹è¿›ç‰ˆè¯„ä¼°ï¼šä½¿ç”¨å¤šä¸ªæ—¶é—´æ­¥ç´¯ç§¯ Loss æ¥é™ä½æ–¹å·®ï¼Œæé«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚
    """
    try:
        from scipy.optimize import linear_sum_assignment
    except ImportError:
        print("Scipy not found, skipping detailed evaluation.")
        return 0.0, {}, 0.0

    model.eval()
    preds, ys_true = [], []
    
    # [ä¿®æ­£ 1] å‰”é™¤ 700, 900ï¼Œåªä¿ç•™ä¿¡å·æœ€å¼ºçš„åŒºé—´
    # eval_timesteps = [300, 400, 500] 
    eval_timesteps = [60, 100, 140] 
    
    # [ä¿®æ­£ 2] å¢åŠ é‡å¤æ¬¡æ•° (è®­ç»ƒæ—¶ä¸ºäº†é€Ÿåº¦å¯ä»¥ç”¨ 3-5 æ¬¡ï¼Œä¸ç”¨ 10 æ¬¡)
    n_repeats = 5
    
    with torch.no_grad():
        for x_0, y_true in loader:
            x_0 = x_0.to(cfg.device)
            batch_size = x_0.size(0)
            
            # (Batch, Num_Classes)
            cumulative_mse = torch.zeros(batch_size, cfg.num_classes, device=cfg.device)
            
            for t_val in eval_timesteps:
                mse_t_sum = torch.zeros(batch_size, cfg.num_classes, device=cfg.device)
                
                # é‡å¤é‡‡æ ·ä»¥æ¶ˆé™¤æ–¹å·®
                for _ in range(n_repeats):
                    noise = torch.randn_like(x_0)
                    current_t = torch.full((batch_size,), t_val, device=cfg.device, dtype=torch.long)
                    x_t = model.dpm_process.q_sample(x_0, current_t, noise)
                    
                    for k in range(cfg.num_classes):
                        y_vec = F.one_hot(torch.full((batch_size,), k, device=x_0.device), cfg.num_classes).float()
                        pred_noise = model.cond_denoiser(x_t, current_t, y_vec)
                        
                        loss = F.mse_loss(pred_noise, noise, reduction='none').view(batch_size, -1).mean(dim=1)
                        mse_t_sum[:, k] += loss
                
                cumulative_mse += (mse_t_sum / n_repeats)

            pred_cluster = torch.argmin(cumulative_mse, dim=1).cpu().numpy()
            # [æ–°å¢è°ƒè¯•æ‰“å°]
            unique_preds, counts = np.unique(pred_cluster, return_counts=True)
            print(f"DEBUG: Predicted Clusters Distribution: {dict(zip(unique_preds, counts))}")
        
            preds.append(pred_cluster)
            ys_true.append(y_true.numpy())

    preds = np.concatenate(preds)
    ys_true = np.concatenate(ys_true)
    
    # --- è®¡ç®—æŒ‡æ ‡ ---
    nmi = NMI(ys_true, preds)
    
    n_classes = cfg.num_classes
    cost_matrix = np.zeros((n_classes, n_classes))
    for i in range(n_classes):
        for j in range(n_classes):
            cost_matrix[i, j] = -np.sum((ys_true == i) & (preds == j))
            
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    cluster2label = {int(c): int(l) for c, l in zip(col_ind, row_ind)}
    aligned_preds = np.array([cluster2label.get(p, 0) for p in preds])
    posterior_acc = np.mean(aligned_preds == ys_true)
    
    return posterior_acc, cluster2label, nmi

def sample_and_save_dpm(denoiser, dpm_process, num_classes, out_path, device, n_per_class=10):
    """ä¿®æ­£åçš„ DPM é€†å‘é‡‡æ ·è¿‡ç¨‹"""
    T = dpm_process.timesteps
    denoiser.eval()
    image_c = dpm_process.image_channels

    with torch.no_grad():
        shape = (n_per_class * num_classes, image_c, 28, 28)
        x_t = torch.randn(shape, device=device) # x_T
        
        # æ„é€ æ¡ä»¶
        y_cond = torch.arange(num_classes).to(device).repeat_interleave(n_per_class).long()
        
        # ä¿®æ­£: å¾ªç¯èŒƒå›´ä» T-1 åˆ° 0
        for i in reversed(range(0, T)):
            t = torch.full((shape[0],), i, device=device, dtype=torch.long)
            
            alpha_t = dpm_process._extract_t(dpm_process.alphas, t, shape)
            one_minus_alpha_t_bar = dpm_process._extract_t(dpm_process.sqrt_one_minus_alphas_cumprod, t, shape)
            
            # é¢„æµ‹å™ªå£°
            pred_noise = denoiser(x_t, t, y_cond)
            
            # è®¡ç®—å‡å€¼ mu_{t-1}
            mu_t_1 = (x_t - (1 - alpha_t) / one_minus_alpha_t_bar * pred_noise) / alpha_t.sqrt()
            
            # è®¡ç®—æ ‡å‡†å·®
            sigma_t_1 = dpm_process._extract_t(dpm_process.posterior_variance, t, shape).sqrt()
            
            # ä¿®æ­£: åªæœ‰å½“ i > 0 æ—¶æ‰æ·»åŠ å™ªå£°
            if i > 0:
                noise = torch.randn_like(x_t)
            else:
                noise = torch.zeros_like(x_t)
                
            x_t = mu_t_1 + sigma_t_1 * noise

        
        save_image(x_t.clamp(-1, 1), out_path, nrow=n_per_class, normalize=True, value_range=(-1, 1))
    print(f"ğŸ’¾ Saved DPM samples to {out_path}")

# -----------------------
# Training Engine
# -----------------------
def run_training_session(model, optimizer, labeled_loader, unlabeled_loader, val_loader, cfg,
                         is_final_training=False, trial_id=None):
    """
    é€šç”¨è®­ç»ƒå‡½æ•°ï¼šå…¼å®¹å…¨ç›‘ç£ã€åŠç›‘ç£ã€æ— ç›‘ç£ã€‚
    é€šè¿‡æ£€æµ‹ loader æ˜¯å¦ä¸º None ä»¥åŠ cfg.alpha_unlabeled æ¥è‡ªåŠ¨åˆ‡æ¢ç­–ç•¥ã€‚
    """
    total_epochs = cfg.final_epochs if is_final_training else cfg.optuna_epochs
    sample_dir = os.path.join(cfg.output_dir, "sample_progress")
    os.makedirs(sample_dir, exist_ok=True)

    metrics = {"Loss": [], "NMI": [], "Acc": []}
    best_val_nmi = -np.inf
    
    # --- 1. è‡ªåŠ¨åˆ¤æ–­è®­ç»ƒæ¨¡å¼ ---
    mode = "UNKNOWN"
    if labeled_loader is not None and unlabeled_loader is not None:
        mode = "SEMI_SUPERVISED"
        print("ğŸš€ æ¨¡å¼æ£€æµ‹: åŠç›‘ç£è®­ç»ƒ (Semi-Supervised)")
    elif labeled_loader is not None and unlabeled_loader is None:
        mode = "SUPERVISED"
        # å¼ºåˆ¶ä¿®æ­£ï¼šå¦‚æœæ²¡æ— æ ‡ç­¾æ•°æ®ï¼Œalpha å¿…é¡»ä¸º 0
        cfg.alpha_unlabeled = 0.0 
        print("ğŸš€ æ¨¡å¼æ£€æµ‹: å…¨ç›‘ç£è®­ç»ƒ (Fully Supervised)")
    elif labeled_loader is None and unlabeled_loader is not None:
        mode = "UNSUPERVISED"
        print("ğŸš€ æ¨¡å¼æ£€æµ‹: æ— ç›‘ç£è®­ç»ƒ (Unsupervised)")
    else:
        raise ValueError("âŒ é”™è¯¯: Labeled å’Œ Unlabeled loader ä¸èƒ½åŒæ—¶ä¸ºç©ºï¼")

    # --- 2. è®­ç»ƒå¾ªç¯ ---
    for epoch in range(1, total_epochs + 1):
        model.train()
        loss_accum = 0.0
        n_batches = 0
        
        # === ç­–ç•¥ A: æ— ç›‘ç£æ¨¡å¼ä¸‹çš„æ¸©åº¦é€€ç« ===
        # æ— ç›‘ç£éœ€è¦æ¿€è¿›çš„é€€ç« (High -> Low)
        if mode == "UNSUPERVISED":
             if epoch > 5:
                cfg.current_gumbel_temp = max(cfg.min_gumbel_temp, cfg.current_gumbel_temp * cfg.gumbel_anneal_rate)
        # åŠç›‘ç£/å…¨ç›‘ç£é€šå¸¸ä¿æŒè¾ƒä½æ¸©åº¦æˆ–ç¼“æ…¢é€€ç«
        elif epoch > total_epochs * 0.5:
             cfg.current_gumbel_temp = max(cfg.min_gumbel_temp, cfg.current_gumbel_temp * 0.995)

        # === ç­–ç•¥ B: åŠç›‘ç£æ¨¡å¼ä¸‹çš„ Warm-up ===
        # å‰ 10 ä¸ª Epoch å¼ºåˆ¶åªçœ‹æœ‰æ ‡ç­¾æ•°æ®
        current_alpha_un = cfg.alpha_unlabeled
        if mode == "SEMI_SUPERVISED" and epoch <= 10:
            current_alpha_un = 0.0
        
        # === 3. æ„é€ é€šç”¨è¿­ä»£å™¨ ===
        # æŠ€å·§ï¼šå°†ä¸åŒçš„ Loader åŒ…è£…æˆç»Ÿä¸€çš„ (batch_lab, batch_un) æ ¼å¼
        if mode == "SEMI_SUPERVISED":
            # å– min lengthï¼Œæˆ–è€…ç”¨ itertools.cycle å¾ªç¯è¾ƒçŸ­çš„é‚£ä¸ª
            iterator = zip(labeled_loader, unlabeled_loader)
            loader_len = len(labeled_loader) # ä»¥æœ‰æ ‡ç­¾çš„ä¸ºå‡†
        elif mode == "SUPERVISED":
            # ä¼ªé€ ä¸€ä¸ªç©ºçš„ unlabeled batch
            iterator = ((batch, None) for batch in labeled_loader)
            loader_len = len(labeled_loader)
        elif mode == "UNSUPERVISED":
            # ä¼ªé€ ä¸€ä¸ªç©ºçš„ labeled batch
            iterator = ((None, batch) for batch in unlabeled_loader)
            loader_len = len(unlabeled_loader)

        # === 4. Batch å¾ªç¯ ===
        for batch_lab, batch_un in iterator:
            optimizer.zero_grad()
            total_loss = torch.tensor(0.0, device=cfg.device)
            resp = None # ç”¨äºæ›´æ–° Prior

            # --- è®¡ç®—æœ‰ç›‘ç£éƒ¨åˆ† ---
            if batch_lab is not None:
                x_lab, y_lab = batch_lab
                x_lab, y_lab = x_lab.to(cfg.device), y_lab.to(cfg.device).long()
                
                # æœ‰æ ‡ç­¾ Loss (å§‹ç»ˆæƒé‡ä¸º 1.0 æˆ–è‡ªå®šä¹‰ alpha_labeled)
                loss_lab, _, _, _, _, _ = model(x_lab, cfg, y_lab)
                total_loss += loss_lab

            # --- è®¡ç®—æ— ç›‘ç£éƒ¨åˆ† ---
            if batch_un is not None and current_alpha_un > 0:
                x_un, _ = batch_un # å¿½ç•¥æ ‡ç­¾
                x_un = x_un.to(cfg.device)
                
                # æ— æ ‡ç­¾ Loss
                loss_un, _, _, _, resp, _ = model(x_un, cfg, None)
                total_loss += current_alpha_un * loss_un
            
            # --- åå‘ä¼ æ’­ ---
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            loss_accum += total_loss.item()
            n_batches += 1

            # EMA æ›´æ–° Prior (ä»…å½“æœ‰æ— ç›‘ç£æ¨æ–­å‘ç”Ÿæ—¶)
            if resp is not None:
                with torch.no_grad():
                    # æ— ç›‘ç£æ¨¡å¼ä¸‹ï¼ŒåŠ¨é‡è®¾ä¸º 0.9 æ›´å¿«å“åº”ï¼Œå¼ºè¿«æ¨¡å‹æ³¨æ„é‚£äº›â€œæ²¡äººé€‰â€çš„ç±»åˆ«
                    momentum = 0.9 if mode == "UNSUPERVISED" else 0.99
                    model.registered_pi.copy_(momentum * model.registered_pi + (1-momentum) * resp.mean(0).detach())

        # === 5. è¯„ä¼°ä¸æ—¥å¿— ===
        # ä½¿ç”¨ä¿®æ­£åçš„ evaluate_model (åŒ…å«é»„é‡‘åŒºé—´å’Œå¤šæ¬¡é‡‡æ ·)
        raw_acc, _, val_nmi = evaluate_model(model, val_loader, cfg)
        
        # è®°å½•æœ€ä½³æ¨¡å‹
        target_metric = raw_acc if mode == "SUPERVISED" else val_nmi
        if target_metric > best_val_nmi:
            best_val_nmi = target_metric
            if is_final_training:
                torch.save(model.state_dict(), os.path.join(cfg.output_dir, "best_model.pt"))

        log_tag = "FINAL" if is_final_training else f"TRIAL-{trial_id}"
        print(f"[{log_tag}] Mode: {mode} | Epoch {epoch} | Loss: {loss_accum/n_batches:.4f} | "
              f"Acc: {raw_acc:.4f} | NMI: {val_nmi:.4f} | Ï„: {cfg.current_gumbel_temp:.3f}")

        # å®šæœŸä¿å­˜å›¾ç‰‡
        if is_final_training and (epoch % 10 == 0 or epoch == total_epochs):
            sample_and_save_dpm(model.cond_denoiser, model.dpm_process, cfg.num_classes,
                                os.path.join(sample_dir, f"epoch_{epoch:03d}.png"), cfg.device)
    
    return best_val_nmi, metrics

def objective(trial):
    cfg = Config()
    cfg.output_dir = "./mDPM_optuna_temp"
    
    # Hyperparameters to tune
    # cfg.unet_base_channels = trial.suggest_categorical("base_channels", [32, 64])
    # å¼ºåˆ¶è®©ç»´åº¦ä¸º32
    cfg.unet_base_channels = 32

    cfg.lambda_entropy = trial.suggest_float("lambda_entropy", 0.1, 5.0)
    cfg.lr = trial.suggest_float("lr", 1e-4, 1e-3, log=True)
    
    model = None
    optimizer = None
    
    try:
        model = mDPM_SemiSup(cfg).to(cfg.device)
        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)
        labeled_loader, unlabeled_loader, val_loader = get_semi_loaders(cfg)

        best_nmi, _ = run_training_session(model, optimizer, labeled_loader, unlabeled_loader, val_loader, cfg,
                                           is_final_training=False, trial_id=trial.number)
        return -best_nmi # Optuna minimizes
        
    except Exception as e:
        print(f"Trial failed: {e}")
        raise optuna.TrialPruned()
    finally:
        # æ˜¾å¼å†…å­˜æ¸…ç†
        del model
        del optimizer
        gc.collect()
        torch.cuda.empty_cache()

def main():
    # ==========================================
    # ğŸ›ï¸ æ§åˆ¶å¼€å…³ï¼šæ˜¯å¦è¿›è¡Œ Optuna è¶…å‚æ•°æœç´¢
    # True  = è¿è¡Œæœç´¢ï¼Œæ‰¾åˆ°æœ€ä¼˜å‚åè®­ç»ƒ (æ…¢)
    # False = è·³è¿‡æœç´¢ï¼Œç›´æ¥ç”¨ Config é»˜è®¤å‚æ•°è®­ç»ƒ (å¿«)
    # ==========================================
    RUN_OPTUNA = False 

    # åˆå§‹åŒ–åŸºç¡€é…ç½®
    cfg = Config()

    if RUN_OPTUNA:
        # --- 1. è¿è¡Œ Optuna æœç´¢ ---
        print("--- Starting Optuna Hyperparameter Search ---")
        study = optuna.create_study(direction="minimize")
        study.optimize(objective, n_trials=5) 
        
        print("Best params found:", study.best_params)
        
        # å°†æœç´¢åˆ°çš„æœ€ä¼˜å‚æ•°è¦†ç›–åˆ° cfg ä¸­
        for k, v in study.best_params.items():
            setattr(cfg, k, v)
            
        # ä¿å­˜æœ€ä¼˜å‚æ•°å¤‡ä»½
        with open(os.path.join(cfg.output_dir, "optuna_best_params.json"), "w") as f:
            json.dump(study.best_params, f, indent=4)
            
    else:
        # --- 2. è·³è¿‡æœç´¢ï¼Œä½¿ç”¨é»˜è®¤/æ‰‹åŠ¨é…ç½® ---
        print("--- Skipping Optuna: Using Manual/Default Config ---")
        
        # [å…³é”®å®‰å…¨è®¾ç½®] 
        # ä¹‹å‰æˆ‘ä»¬åœ¨ objective é‡Œå¼ºåˆ¶æ”¹æˆäº† 32 ä»¥é˜²çˆ†æ˜¾å­˜
        # å¦‚æœè·³è¿‡ Optunaï¼Œå¿…é¡»åœ¨è¿™é‡Œæ‰‹åŠ¨è®¾ä¸º 32ï¼Œå¦åˆ™ä¼šè¯» common_dpm é‡Œçš„é»˜è®¤å€¼ 64
        cfg.unet_base_channels = 32
        
        # ä½ ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæ‰‹åŠ¨å¾®è°ƒå…¶ä»–å‚æ•°ï¼Œä¾‹å¦‚ï¼š
        # cfg.lr = 1e-3
        # cfg.lambda_entropy = 2.0
        
    # --- 3. å¼€å§‹æœ€ç»ˆè®­ç»ƒ ---
    print("\n" + "="*30)
    print("--- Starting Final Training ---")
    print(f"Config: Channels={cfg.unet_base_channels}, LR={cfg.lr}")
    print("="*30 + "\n")

    model = mDPM_SemiSup(cfg).to(cfg.device)
    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)
    labeled_loader, unlabeled_loader, val_loader = get_semi_loaders(cfg)
    
    # è¿è¡Œè®­ç»ƒ
    run_training_session(model, optimizer, labeled_loader, unlabeled_loader, val_loader, cfg, is_final_training=True)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), os.path.join(cfg.output_dir, "final_model.pt"))
    print(f"âœ… Done. Model saved to {cfg.output_dir}")

if __name__ == "__main__":
    main()