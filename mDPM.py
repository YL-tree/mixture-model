# mDPM.py
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import optuna
import os
import gc
import numpy as np
from scipy.optimize import linear_sum_assignment
from torchvision.utils import save_image
import itertools
from common_dpm import *

# -----------------------
# Model Definition
# -----------------------
class mDPM_SemiSup(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.cond_denoiser = ConditionalUnet(
            in_channels=cfg.image_channels,
            base_channels=cfg.unet_base_channels,
            num_classes=cfg.num_classes,
            time_emb_dim=cfg.unet_time_emb_dim
        )
        self.dpm_process = DPMForwardProcess(
            timesteps=cfg.timesteps,
            schedule='linear',
            image_channels=cfg.image_channels
        )
        # ç±»åˆ«å…ˆéªŒï¼šåˆå§‹åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ
        self.register_buffer('registered_pi', torch.ones(cfg.num_classes) / cfg.num_classes)
        
    def estimate_posterior_logits(self, x_0, cfg, scale_factor=1.0):
        batch_size = x_0.size(0)
        num_classes = cfg.num_classes
        M = cfg.posterior_sample_steps
        
        # Log Likelihood ç´¯åŠ å™¨
        accum_neg_mse = torch.zeros(batch_size, num_classes, device=x_0.device)
        
        with torch.no_grad():
            for _ in range(M):
                # é‡‡æ ·æ—¶é—´æ­¥ (è¦†ç›–ä¸­é—´åŒºåŸŸ)
                t = torch.randint(100, 900, (batch_size,), device=x_0.device).long()
                noise = torch.randn_like(x_0)
                x_t = self.dpm_process.q_sample(x_0, t, noise)
                
                for k in range(num_classes):
                    y_cond = torch.full((batch_size,), k, device=x_0.device, dtype=torch.long)
                    y_onehot = F.one_hot(y_cond, num_classes=num_classes).float()
                    pred_noise = self.cond_denoiser(x_t, t, y_onehot)
                    
                    mse = F.mse_loss(pred_noise, noise, reduction='none').view(batch_size, -1).mean(dim=1)
                    accum_neg_mse[:, k] += -mse

        avg_neg_mse = accum_neg_mse / M
        log_pi = torch.log(torch.clamp(self.registered_pi, min=1e-6)).unsqueeze(0)
        
        # Logits = Prior + Scale * Likelihood
        final_logits = log_pi + (avg_neg_mse * scale_factor)
        return final_logits

    def forward(self, x_0, cfg, y=None, current_scale=100.0, current_lambda=0.0, threshold=0.95):
        """
        å‰å‘ä¼ æ’­ï¼šæ ¹æ® y æ˜¯å¦å­˜åœ¨ï¼Œè‡ªåŠ¨é€‰æ‹© Supervised æˆ– FixMatch è·¯å¾„
        """
        batch_size = x_0.size(0)

        # -------------------
        # Path A: ç›‘ç£æ¨¡å¼ (æœ‰æ ‡ç­¾) - è€å¸ˆæ•™
        # -------------------
        if y is not None:
            t = torch.randint(0, cfg.timesteps, (batch_size,), device=x_0.device).long()
            noise = torch.randn_like(x_0)
            x_t = self.dpm_process.q_sample(x_0, t, noise)
            
            y_onehot = F.one_hot(y, num_classes=cfg.num_classes).float()
            pred_noise = self.cond_denoiser(x_t, t, y_onehot)
            dpm_loss = F.mse_loss(pred_noise, noise, reduction='mean') 
            
            # è¿”å›æ ¼å¼: (total_loss, neg_elbo, dpm_loss, mask_rate, resp, None)
            return dpm_loss, -dpm_loss.item(), dpm_loss.item(), 1.0, None, None
            
        # -------------------
        # Path B: æ— ç›‘ç£æ¨¡å¼ (FixMatch ç­–ç•¥) - ä¸¥å‰çš„è‡ªå­¦
        # -------------------
        else:
            # 1. E-Step: ä¼°ç®—å±äºå“ªä¸€ç±»
            logits = self.estimate_posterior_logits(x_0, cfg, scale_factor=current_scale)
            resp = F.softmax(logits, dim=1) 
            
            # 2. è·å–ç½®ä¿¡åº¦å’Œä¼ªæ ‡ç­¾
            max_probs, pseudo_labels = resp.max(dim=1)
            
            # 3. [æ ¸å¿ƒç­–ç•¥] é˜ˆå€¼è¿‡æ»¤ (Thresholding)
            # åªæœ‰ç½®ä¿¡åº¦æé«˜ (>0.95) çš„æ ·æœ¬æ‰å…è®¸è¿›å…¥è®­ç»ƒ
            # "è¦ä¹ˆéå¸¸ç¡®å®šï¼Œè¦ä¹ˆé—­å˜´"
            # åŠç›‘ç£çš„æ—¶å€™ä¸º0.95
            mask = (max_probs >= threshold).float()

            # 4. [æ ¸å¿ƒç­–ç•¥] Hard Label
            # å³ä½¿é€šè¿‡äº†é˜ˆå€¼ï¼Œä¹Ÿåªç»™ One-hot æ ‡ç­¾ï¼Œç»ä¸ç»™æ¨¡ç³Šçš„ Soft Label
            # é˜²æ­¢ç‰¹å¾ç©ºé—´è¢«ç¨€é‡Š
            y_hard = F.one_hot(pseudo_labels, num_classes=cfg.num_classes).float()
            
            # 5. M-Step: å¸¦ç€ä¼ªæ ‡ç­¾å»è®­ç»ƒ
            t_train = torch.randint(0, cfg.timesteps, (batch_size,), device=x_0.device).long()
            noise = torch.randn_like(x_0)
            x_t_train = self.dpm_process.q_sample(x_0, t_train, noise)
            
            # ä¼ å…¥ Hard Label
            pred_noise = self.cond_denoiser(x_t_train, t_train, y_hard)
            
            # 6. è®¡ç®— Loss (åº”ç”¨ Mask)
            loss_per_sample = F.mse_loss(pred_noise, noise, reduction='none').view(batch_size, -1).mean(dim=1)
            
            # åªæœ‰ mask=1 çš„æ ·æœ¬äº§ç”Ÿæ¢¯åº¦
            dpm_loss = (loss_per_sample * mask).sum() / (mask.sum() + 1e-8)
            
            # 7. è¾…åŠ©æŸå¤± (Entropy) - å¯é€‰ï¼Œæƒé‡å¾ˆä½
            entropy = -(resp * torch.log(resp + 1e-8)).sum(dim=1).mean()
            total_loss = dpm_loss + current_lambda * entropy

            # [FATAL FIX] å½»åº•å†»ç»“ Prior æ›´æ–°
            # åœ¨åŠç›‘ç£åˆæœŸï¼Œæ›´æ–° Prior ææ˜“å¯¼è‡´ Mode Collapse (é©¬å¤ªæ•ˆåº”)ã€‚
            # æˆ‘ä»¬å¼ºåˆ¶ä¿æŒ Prior ä¸ºå‡åŒ€åˆ†å¸ƒï¼Œè®© MSE å†³å®šä¸€åˆ‡ã€‚
            # if self.training:
            #     with torch.no_grad():
            #         self.registered_pi.copy_(...) <--- æ³¨é‡Šæ‰äº†

            mask_rate = mask.mean().item()
            return total_loss, -total_loss.item(), dpm_loss.item(), mask_rate, resp.detach(), None

# -----------------------
# Evaluation Utils
# -----------------------
def evaluate_model(model, loader, cfg):
    try:
        from scipy.optimize import linear_sum_assignment
    except ImportError:
        return 0.0, {}, 0.0

    model.eval()
    preds, ys_true = [], []
    
    # ç®€åŒ–è¯„ä¼°ï¼šåªç”¨ä¸­é—´æ—¶åˆ»
    eval_timesteps = [500] 
    n_repeats = 3 
    
    with torch.no_grad():
        for x_0, y_true in loader:
            x_0 = x_0.to(cfg.device)
            batch_size = x_0.size(0)
            cumulative_mse = torch.zeros(batch_size, cfg.num_classes, device=cfg.device)
            
            for t_val in eval_timesteps:
                mse_t_sum = torch.zeros(batch_size, cfg.num_classes, device=cfg.device)
                for _ in range(n_repeats):
                    noise = torch.randn_like(x_0)
                    current_t = torch.full((batch_size,), t_val, device=cfg.device, dtype=torch.long)
                    x_t = model.dpm_process.q_sample(x_0, current_t, noise)
                    for k in range(cfg.num_classes):
                        y_vec = F.one_hot(torch.full((batch_size,), k, device=x_0.device), cfg.num_classes).float()
                        pred = model.cond_denoiser(x_t, current_t, y_vec)
                        loss = F.mse_loss(pred, noise, reduction='none').view(batch_size, -1).mean(dim=1)
                        mse_t_sum[:, k] += loss
                cumulative_mse += (mse_t_sum / n_repeats)

            pred_cluster = torch.argmin(cumulative_mse, dim=1).cpu().numpy()
            preds.append(pred_cluster)
            ys_true.append(y_true.numpy())

    preds = np.concatenate(preds)
    ys_true = np.concatenate(ys_true)
    
    nmi = 0.0 # NMI(ys_true, preds) # æš‚æ—¶çœç•¥ NMI ä»¥åŠ é€Ÿ
    
    # è®¡ç®— Acc
    n_classes = cfg.num_classes
    cost_matrix = np.zeros((n_classes, n_classes))
    for i in range(n_classes):
        for j in range(n_classes):
            cost_matrix[i, j] = -np.sum((ys_true == i) & (preds == j))
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    cluster2label = {int(c): int(l) for c, l in zip(col_ind, row_ind)}
    aligned_preds = np.array([cluster2label.get(p, 0) for p in preds])
    acc = np.mean(aligned_preds == ys_true)
    
    return acc, cluster2label, nmi

def sample_and_save_dpm(denoiser, dpm_process, num_classes, out_path, device, n_per_class=10):
    """ç”Ÿæˆå›¾åƒç½‘æ ¼ï¼šæ¯è¡Œä¸€ä¸ªç±»åˆ«"""
    T = dpm_process.timesteps
    denoiser.eval()
    image_c = dpm_process.image_channels

    with torch.no_grad():
        # å½¢çŠ¶: (100, 1, 28, 28)
        shape = (n_per_class * num_classes, image_c, 28, 28)
        x_t = torch.randn(shape, device=device)
        # ç”Ÿæˆæ¡ä»¶: [0,0,..,0, 1,1,..,1, ...]
        y_cond = torch.arange(num_classes).to(device).repeat_interleave(n_per_class).long()
        y_cond_vec = F.one_hot(y_cond, num_classes).float() # Hard Label Cond
        
        for i in reversed(range(0, T)):
            t = torch.full((shape[0],), i, device=device, dtype=torch.long)
            
            # æ ‡å‡† DDPM é‡‡æ ·æ­¥
            alpha_t = dpm_process._extract_t(dpm_process.alphas, t, shape)
            one_minus_alpha_t_bar = dpm_process._extract_t(dpm_process.sqrt_one_minus_alphas_cumprod, t, shape)
            
            pred_noise = denoiser(x_t, t, y_cond_vec)
            
            mu_t_1 = (x_t - (1 - alpha_t) / one_minus_alpha_t_bar * pred_noise) / alpha_t.sqrt()
            sigma_t_1 = dpm_process._extract_t(dpm_process.posterior_variance, t, shape).sqrt()
            
            if i > 0:
                noise = torch.randn_like(x_t)
            else:
                noise = torch.zeros_like(x_t)
            x_t = mu_t_1 + sigma_t_1 * noise

        save_image(x_t.clamp(-1, 1), out_path, nrow=n_per_class, normalize=True, value_range=(-1, 1))
    print(f"   [Visual] Samples saved to {out_path}")

# -----------------------
# Training Engine
# -----------------------
def run_training_session(model, optimizer, labeled_loader, unlabeled_loader, val_loader, cfg,
                         is_final_training=False, trial_id=None, resume_path=None):
    
    total_epochs = cfg.final_epochs
    sample_dir = os.path.join(cfg.output_dir, "sample_progress")
    os.makedirs(sample_dir, exist_ok=True)

    start_epoch = 1
    best_val_acc = 0.0

    # Resume Logic
    if resume_path and os.path.exists(resume_path):
        checkpoint = torch.load(resume_path, map_location=cfg.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_val_acc = checkpoint.get('best_nmi', 0.0)
        print(f"ğŸ”„ Resumed at Ep {start_epoch}")

    # ==========================================
    # [ä¿®å¤] å®Œå–„çš„æ¨¡å¼æ£€æµ‹é€»è¾‘
    # ==========================================
    mode = "UNKNOWN"
    if labeled_loader is not None and unlabeled_loader is not None: 
        mode = "SEMI_SUPERVISED"
    elif labeled_loader is not None: 
        mode = "SUPERVISED"
        cfg.alpha_unlabeled = 0.0 # å¼ºåˆ¶å…³é—­æ— ç›‘ç£æƒé‡
    elif unlabeled_loader is not None: 
        # è¿™ç§æƒ…å†µå°±æ˜¯ä½ ç°åœ¨éœ€è¦çš„
        mode = "UNSUPERVISED"
        cfg.alpha_unlabeled = 1.0 # å¼ºåˆ¶å¼€å¯æ— ç›‘ç£æƒé‡
    
    print(f"ğŸš€ Training Mode: {mode}")

    for epoch in range(start_epoch, total_epochs + 1):
        progress = (epoch - 1) / total_epochs
        
        # Scale: ä¿æŒ 300 -> 600
        dynamic_scale = 300.0 + (600.0 - 300.0) * progress
        
        # [æ–°å¢] Threshold è¯¾ç¨‹å­¦ä¹ : 0.70 -> 0.95
        # æ— ç›‘ç£èµ·æ­¥éš¾ï¼Œå…ˆé™ä½é—¨æ§›
        dynamic_threshold = 0.70 + (0.95 - 0.70) * progress

        if epoch % 5 == 0 or epoch == 1:
            print(f"ğŸ”¥ [Scheduler] Ep {epoch}: Scale={dynamic_scale:.1f}, Thres={dynamic_threshold:.2f}")

        model.train()
        loss_accum = 0.0
        mask_rate_accum = 0.0
        n_batches = 0
        
        # Warm-up (ä»…åœ¨åŠç›‘ç£æ—¶ç”Ÿæ•ˆï¼Œæ— ç›‘ç£ä¸è·³è¿‡)
        current_alpha = cfg.alpha_unlabeled
        if mode == "SEMI_SUPERVISED" and epoch <= 5: 
            current_alpha = 0.0
        
        # ==========================================
        # [ä¿®å¤] Iterator åˆ†å‘é€»è¾‘
        # ==========================================
        if mode == "SEMI_SUPERVISED": 
            iterator = zip(itertools.cycle(labeled_loader), unlabeled_loader)
        elif mode == "SUPERVISED":
            iterator = ((batch, None) for batch in labeled_loader)
        elif mode == "UNSUPERVISED":
            # çº¯æ— ç›‘ç£ï¼šç¬¬ä¸€ä¸ªä½ç½®(labeled)ä¼  Noneï¼Œç¬¬äºŒä¸ªä½ç½®(unlabeled)ä¼ æ•°æ®
            iterator = ((None, batch) for batch in unlabeled_loader)

        for batch_lab, batch_un in iterator:
            optimizer.zero_grad()
            total_loss = torch.tensor(0.0, device=cfg.device)

            # A. æœ‰æ ‡ç­¾éƒ¨åˆ† (UNSUPERVISED æ¨¡å¼ä¸‹ batch_lab ä¸º Noneï¼Œä¼šè‡ªåŠ¨è·³è¿‡)
            if batch_lab is not None:
                x, y = batch_lab
                x, y = x.to(cfg.device), y.to(cfg.device).long()
                l_sup, _, _, _, _, _ = model(x, cfg, y=y)
                total_loss += l_sup

            # B. æ— æ ‡ç­¾éƒ¨åˆ† (SUPERVISED æ¨¡å¼ä¸‹ batch_un ä¸º Noneï¼Œä¼šè‡ªåŠ¨è·³è¿‡)
            if batch_un is not None and current_alpha > 0:
                x_un, _ = batch_un
                x_un = x_un.to(cfg.device)
                
                # ä¼ å…¥ dynamic_threshold
                l_unsup, _, _, mask_rate, _, _ = model(x_un, cfg, y=None, 
                                                       current_scale=dynamic_scale,
                                                       current_lambda=0.01,
                                                       threshold=dynamic_threshold)
                
                total_loss += current_alpha * l_unsup
                mask_rate_accum += mask_rate

            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            loss_accum += total_loss.item()
            n_batches += 1

        # Validation
        val_acc, _, _ = evaluate_model(model, val_loader, cfg)
        
        # Checkpointing
        ckpt = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_nmi': max(val_acc, best_val_acc)
        }
        torch.save(ckpt, os.path.join(cfg.output_dir, "checkpoint_last.pt"))
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(ckpt, os.path.join(cfg.output_dir, "best_model.pt"))
            print(f"   â˜… New Best! Acc: {best_val_acc:.4f}")

        avg_mask = mask_rate_accum / n_batches if n_batches > 0 else 0
        print(f"Ep {epoch} | Loss: {loss_accum/n_batches:.4f} | Val Acc: {val_acc:.4f} | Pass: {avg_mask*100:.1f}%")

        if epoch % 1 == 0:
            sample_and_save_dpm(model.cond_denoiser, model.dpm_process, cfg.num_classes,
                                os.path.join(sample_dir, f"epoch_{epoch:03d}.png"), cfg.device)
    
    return best_val_acc, {}
    
def main():
    cfg = Config()
    # å¼ºåˆ¶è¦†ç›–é…ç½®ä»¥ç¡®ä¿ FixMatch ç”Ÿæ•ˆ
    cfg.alpha_unlabeled = 0.1  # æƒé‡å¿…é¡»å°
    cfg.posterior_sample_steps = 5 # é‡‡æ ·å¿…é¡»å‡†
    
    print("="*30)
    print(f"--- FixMatch Training (Threshold=0.95) ---")
    print(f"Config: LR={cfg.lr}, Alpha={cfg.alpha_unlabeled}")
    print("="*30)

    model = mDPM_SemiSup(cfg).to(cfg.device)
    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)
    labeled_loader, unlabeled_loader, val_loader = get_semi_loaders(cfg)
    
    # é»˜è®¤å°è¯• Resume
    resume_path = os.path.join(cfg.output_dir, "checkpoint_last.pt")
    
    run_training_session(
        model, optimizer, labeled_loader, unlabeled_loader, val_loader, cfg, 
        is_final_training=True,
        resume_path=resume_path
    )

if __name__ == "__main__":
    main()